# Optional dependency check (avoid installing into /home on clusters)
import importlib.util
print("accelerate available:", importlib.util.find_spec("accelerate") is not None)



import os

# Fix cluster home quota: redirect caches (edit path if needed)
os.environ.setdefault("ROLE_REP_CACHE_DIR", f"/projects/JeFeSpace/KLM/cache/{os.environ.get('USER','user')}/role-rep")
os.environ.setdefault("HF_HOME", os.path.join(os.environ["ROLE_REP_CACHE_DIR"], "hf"))
os.environ.setdefault("HUGGINGFACE_HUB_CACHE", os.path.join(os.environ["HF_HOME"], "hub"))
os.environ.setdefault("TRANSFORMERS_CACHE", os.path.join(os.environ["HF_HOME"], "transformers"))
os.environ.setdefault("TORCH_HOME", os.path.join(os.environ["ROLE_REP_CACHE_DIR"], "torch"))
os.environ.setdefault("XDG_CACHE_HOME", os.path.join(os.environ["ROLE_REP_CACHE_DIR"], "xdg"))

# Avoid widget/progress-bar issues + HF xet
os.environ["HF_HUB_DISABLE_PROGRESS_BARS"] = "1"
os.environ["DISABLE_TQDM"] = "1"
os.environ["HF_HUB_DISABLE_XET"] = "1"

import getpass
if not (os.environ.get("HF_TOKEN") or os.environ.get("HUGGINGFACE_HUB_TOKEN")):
    os.environ["HF_TOKEN"] = getpass.getpass("HF token: ")
    os.environ["HUGGINGFACE_HUB_TOKEN"] = os.environ["HF_TOKEN"]








# %%capture
# Core
import json
import math
import os
import sys
from dataclasses import dataclass
from typing import Dict, List

# --- Cache + auth (prevents /home quota issues on clusters) ---
# You can override explicitly with ROLE_REP_CACHE_DIR.
ROLE_REP_CACHE_DIR = os.environ.get("ROLE_REP_CACHE_DIR")
if ROLE_REP_CACHE_DIR is None:
    user = os.environ.get("USER", "user")
    if os.path.isdir("/projects/JeFeSpace/KLM"):
        ROLE_REP_CACHE_DIR = f"/projects/JeFeSpace/KLM/cache/{user}/role-rep"
    elif os.path.isdir("/scratch"):
        ROLE_REP_CACHE_DIR = f"/scratch/{user}/role-rep-cache"

if ROLE_REP_CACHE_DIR:
    os.makedirs(ROLE_REP_CACHE_DIR, exist_ok=True)
    os.environ.setdefault("HF_HOME", os.path.join(ROLE_REP_CACHE_DIR, "hf"))
    os.environ.setdefault("HUGGINGFACE_HUB_CACHE", os.path.join(os.environ["HF_HOME"], "hub"))
    os.environ.setdefault("TRANSFORMERS_CACHE", os.path.join(os.environ["HF_HOME"], "transformers"))
    os.environ.setdefault("TORCH_HOME", os.path.join(ROLE_REP_CACHE_DIR, "torch"))
    os.environ.setdefault("XDG_CACHE_HOME", os.path.join(ROLE_REP_CACHE_DIR, "xdg"))

# Reduce notebook widget/progress issues
os.environ.setdefault("HF_HUB_DISABLE_PROGRESS_BARS", "1")
# Avoid HF "xet" download path (often problematic on clusters)
os.environ.setdefault("HF_HUB_DISABLE_XET", "1")

# If you installed packages into project space (e.g., accelerate), add that site-packages BEFORE importing transformers.
ROLE_REP_SITE_PACKAGES = os.environ.get(
    "ROLE_REP_SITE_PACKAGES",
    "/projects/JeFeSpace/KLM/pip_local/lib/python3.12/site-packages",
)
if os.path.isdir(ROLE_REP_SITE_PACKAGES) and ROLE_REP_SITE_PACKAGES not in sys.path:
    sys.path.insert(0, ROLE_REP_SITE_PACKAGES)


# If you exported HF_TOKEN/HUGGINGFACE_HUB_TOKEN in your shell, this will be picked up automatically.
HF_TOKEN = os.environ.get("HF_TOKEN") or os.environ.get("HUGGINGFACE_HUB_TOKEN")

# ML / viz
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score

# Hugging Face
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# Repro
np.random.seed(7)
torch.manual_seed(7)






# Transcript: long two-person dialogue (Alice/Bob)
transcript_turns = [
    "Alice: Hey Bob, before we jump in, I wanted to revisit the design proposal.",
    "Bob: Sure, I skimmed it last night; the latency targets look ambitious.",
    "Alice: The client asked for sub-200ms p95; I think batching can get us there.",
    "Bob: Batching helps, but the cache invalidation could become tricky.",
    "Alice: We can constrain invalidation to product-level keys instead of per-user.",
    "Bob: That might reduce precision, though; would that impact personalization?",
    "Alice: Some, but we can re-rank on the client side for the top 10 results.",
    "Bob: Okay, so you are proposing a hybrid: coarse cache, fine rerank.",
    "Alice: Exactly. And we should log enough to measure drift each week.",
    "Bob: Logging is fine, but data retention policy caps at 30 days.",
    "Alice: Right, I can summarize weekly aggregates and delete raw events.",
    "Bob: Great. Also, the new API endpoint needs a version bump.",
    "Alice: v3 seems reasonable; we can keep v2 for a deprecation window.",
    "Bob: Then we need a migration guide; I can draft it.",
    "Alice: Thanks. Another point: the search index rebuild takes 6 hours.",
    "Bob: Maybe we can parallelize by shard and compress the postings lists.",
    "Alice: If we compress too much, we might slow decoding at query time.",
    "Bob: True; we could trade storage for CPU if latency budget allows.",
    "Alice: We'll benchmark both. Also, what about adding synonyms?",
    "Bob: Synonyms help recall, but they increase false positives.",
    "Alice: We'll tune the threshold and evaluate per-category.",
    "Bob: Sounds good. On another note, QA reported flaky tests.",
    "Alice: I saw that; I think the mock clock isn't resetting in CI.",
    "Bob: I can isolate those tests and add a fixture.",
    "Alice: Appreciate it. Lastly, are we aligned on the rollout plan?",
    "Bob: Staged rollout: internal, then 5% external, then 50%.",
    "Alice: And we monitor error rates and rollback if p95 spikes.",
    "Bob: Yes. I'll write the runbook.",
    "Alice: Great, I'll update the proposal and send it today.",
    "Bob: Perfect; I will review as soon as it lands.",
    "Alice: Thanks, Bob.",
    "Bob: Thanks, Alice."
]

transcript = "\n".join(transcript_turns)
transcript


# Model config: Qwen 7B + Qwen 2.5 + Llama 3.1 8B
# These are HF names; swap to local paths if you have them.

MODEL_NAMES = {
    "qwen_7b": os.environ.get("QWEN_7B_MODEL", "Qwen/Qwen2-7B-Instruct"),
    "qwen_2_5": os.environ.get("QWEN_25_MODEL", "Qwen/Qwen2.5-7B-Instruct"),
    # NOTE: official HF repo name often includes "Meta-" prefix
    "llama_3_1_8b": os.environ.get("LLAMA_31_8B_MODEL", "meta-llama/Meta-Llama-3.1-8B-Instruct"),
}

# Some clusters ship an older transformers that can't parse Llama-3.1 rope_scaling.
# This shim overrides rope_scaling into the older (type,factor) format so the model can load.
MODEL_LOAD_KWARGS = {
    "llama_3_1_8b": {
        "rope_scaling": {"type": "linear", "factor": 8.0},
    }
}

# Layers to probe: 0, 25%, 50%, 75%, 100%
LAYER_FRACTIONS = [0.0, 0.25, 0.5, 0.75, 1.0]

# Token positions to probe
# NOTE: For speaker tags, tokenization differs (Llama uses sentencepiece), so we locate them by
# matching token-id subsequences rather than string matching.
POSITION_SPECS = {
    "final": {"type": "final"},
    "speaker_tag": {"type": "speaker_tag", "values": ["Alice", "Bob"]},
    "verb": {"type": "string", "values": ["am", "will", "can", "should", "think", "like", "want"]},
}



@dataclass
class ModelBundle:
    name: str
    tokenizer: AutoTokenizer
    model: AutoModelForCausalLM


def _best_dtype():
    # Use bf16 on A100/H100 when available; otherwise fall back.
    if torch.cuda.is_available():
        try:
            major, _minor = torch.cuda.get_device_capability(0)
            if major >= 8:  # Ampere+
                return torch.bfloat16
        except Exception:
            pass
        return torch.float16
    return torch.float32


def _load_llama_config_with_rope_compat(model_id: str, cache_dir: str | None):
    """Patch Llama-3.1 rope_scaling for older transformers.

    Older `transformers` expects `rope_scaling` to be exactly:
      {"type": <str>, "factor": <float>}
    Llama-3.1 config uses a richer schema (rope_type/low_freq_factor/etc.).
    """
    import json
    from transformers.utils import cached_file
    from transformers.models.llama.configuration_llama import LlamaConfig

    cfg_path = cached_file(model_id, "config.json", cache_dir=cache_dir, token=HF_TOKEN)
    with open(cfg_path, "r") as f:
        cfg = json.load(f)

    rs = cfg.get("rope_scaling")
    if isinstance(rs, dict) and ("type" not in rs) and ("rope_type" in rs):
        cfg["rope_scaling"] = {"type": "linear", "factor": float(rs.get("factor", 8.0))}

    # Build a concrete config object directly (AutoConfig.from_dict may not exist on older installs)
    return LlamaConfig(**cfg)


def load_model(name: str, model_kwargs: Dict | None = None) -> ModelBundle:
    import importlib.util

    cache_dir = os.environ.get("TRANSFORMERS_CACHE") or os.environ.get("HF_HOME")
    dtype = _best_dtype()
    model_kwargs = model_kwargs or {}

    tokenizer = AutoTokenizer.from_pretrained(
        name,
        cache_dir=cache_dir,
        token=HF_TOKEN,
    )

    have_accelerate = importlib.util.find_spec("accelerate") is not None
    device_map = "auto" if (torch.cuda.is_available() and have_accelerate) else None
    low_cpu_mem_usage = True if have_accelerate else False

    try:
        model = AutoModelForCausalLM.from_pretrained(
            name,
            cache_dir=cache_dir,
            device_map=device_map,
            torch_dtype=dtype,
            low_cpu_mem_usage=low_cpu_mem_usage,
            token=HF_TOKEN,
            **model_kwargs,
        )
    except ValueError as e:
        # Handle Llama-3.1 rope_scaling schema mismatch on older transformers
        if "rope_scaling" in str(e) and "type" in str(e) and "factor" in str(e):
            cfg = _load_llama_config_with_rope_compat(name, cache_dir)
            model = AutoModelForCausalLM.from_pretrained(
                name,
                cache_dir=cache_dir,
                config=cfg,
                device_map=device_map,
                torch_dtype=dtype,
                low_cpu_mem_usage=low_cpu_mem_usage,
                token=HF_TOKEN,
            )
        else:
            raise

    if torch.cuda.is_available() and device_map is None:
        model = model.to("cuda")

    model.config.output_hidden_states = True
    model.eval()

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    return ModelBundle(name=name, tokenizer=tokenizer, model=model)


def select_layers(num_layers: int, fractions: List[float]) -> List[int]:
    indices = []
    for f in fractions:
        idx = int(round(f * num_layers))
        idx = max(0, min(num_layers, idx))
        indices.append(idx)
    return sorted(list(dict.fromkeys(indices)))


def _find_subsequence_positions(sequence: List[int], pattern: List[int]) -> List[int]:
    if not pattern or len(pattern) > len(sequence):
        return []
    out = []
    m = len(pattern)
    for i in range(len(sequence) - m + 1):
        if sequence[i : i + m] == pattern:
            out.append(i)
    return out


def _norm_token(t: str) -> str:
    # Works for GPT2 BPE (Ġ) and sentencepiece (▁)
    return t.strip().lstrip("Ġ").lstrip("▁").strip().lower()


def find_positions(tokens: List[str], spec: Dict, tokenizer=None, input_ids=None) -> List[int]:
    t = spec["type"]

    if t == "final":
        return [len(tokens) - 1]

    if t == "speaker_tag":
        if tokenizer is None or input_ids is None:
            return []
        seq = input_ids[0].tolist() if hasattr(input_ids, "shape") else list(input_ids)
        positions = []
        for name in spec.get("values", []):
            pat = tokenizer.encode(name, add_special_tokens=False)
            positions.extend(_find_subsequence_positions(seq, pat))
        return sorted(list(dict.fromkeys(positions)))

    if t == "string":
        values = {v.lower() for v in spec.get("values", [])}
        hits = [i for i, tok in enumerate(tokens) if _norm_token(tok) in values]
        return hits

    return []


def label_speakers(tokens: List[str], raw_text: str) -> np.ndarray:
    labels = []
    last_speaker = None
    cursor = 0

    for tok in tokens:
        window = raw_text[cursor: cursor + 200]
        if "Alice:" in window and ("Bob:" not in window or window.index("Alice:") < window.index("Bob:")):
            last_speaker = "Alice"
        if "Bob:" in window and ("Alice:" not in window or window.index("Bob:") < window.index("Alice:")):
            last_speaker = "Bob"

        labels.append(last_speaker if last_speaker else "Unknown")

        tok_clean = tok.replace("Ġ", " ")
        if tok_clean in raw_text[cursor:]:
            cursor = raw_text.index(tok_clean, cursor) + len(tok_clean)
        else:
            cursor = min(cursor + max(1, len(tok_clean)), len(raw_text))

    return np.array(labels)



def _build_input_ids_and_labels(tokenizer, turns: List[str]):
    """Tokenize turn-by-turn to get robust speaker labels across tokenizers (BPE/SentencePiece)."""
    all_ids: List[int] = []
    all_labels: List[str] = []

    for i, turn in enumerate(turns):
        if turn.startswith("Alice:"):
            sp = "Alice"
        elif turn.startswith("Bob:"):
            sp = "Bob"
        else:
            sp = "Unknown"

        ids = tokenizer(turn, add_special_tokens=False).input_ids
        all_ids.extend(ids)
        all_labels.extend([sp] * len(ids))

        # Add newline separator between turns
        if i < len(turns) - 1:
            nl_ids = tokenizer("\n", add_special_tokens=False).input_ids
            all_ids.extend(nl_ids)
            all_labels.extend([sp] * len(nl_ids))

    input_ids = torch.tensor([all_ids], dtype=torch.long)
    attention_mask = torch.ones_like(input_ids)
    labels = np.array(all_labels)
    return input_ids, attention_mask, labels


def run_analysis_for_model(bundle: ModelBundle) -> Dict:
    tokenizer = bundle.tokenizer
    model = bundle.model

    input_ids, attention_mask, speaker_labels = _build_input_ids_and_labels(tokenizer, transcript_turns)

    # Move inputs to the model device when possible
    device = next(model.parameters()).device
    input_ids = input_ids.to(device)
    attention_mask = attention_mask.to(device)

    with torch.no_grad():
        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=True,
            return_dict=True,
        )

    hidden_states = outputs.hidden_states
    if hidden_states is None:
        raise RuntimeError("Model did not return hidden states; ensure output_hidden_states=True")

    num_layers = len(hidden_states) - 1
    layer_indices = select_layers(num_layers, LAYER_FRACTIONS)

    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].detach().cpu())
    positions = {
        name: find_positions(tokens, spec, tokenizer=tokenizer, input_ids=input_ids.detach().cpu())
        for name, spec in POSITION_SPECS.items()
    }

    data = {}
    for layer in layer_indices:
        hs = hidden_states[layer][0].detach().cpu().numpy()  # [seq_len, hidden_dim]
        for pos_name, pos_indices in positions.items():
            pos_indices = [p for p in pos_indices if p < hs.shape[0]]
            if not pos_indices:
                continue
            X = hs[pos_indices]
            y = speaker_labels[pos_indices]
            mask = np.isin(y, ["Alice", "Bob"])
            X = X[mask]
            y = y[mask]
            if len(y) < 4:
                continue
            data[(layer, pos_name)] = (X, y)

    return {
        "model_name": bundle.name,
        "tokens": tokens,
        "layers": layer_indices,
        "positions": positions,
        "speaker_labels": speaker_labels,
        "data": data,
    }



# Run analysis model-by-model (avoids loading all 7B/8B models at once)

results = {}
for key, name in MODEL_NAMES.items():
    print(f"Loading {key}: {name}")
    bundle = load_model(name, model_kwargs=MODEL_LOAD_KWARGS.get(key))
    try:
        results[key] = run_analysis_for_model(bundle)
    finally:
        # Free GPU/CPU memory before next model
        try:
            del bundle.model
        except Exception:
            pass
        del bundle
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

list(results.keys())


def plot_pca_tsne(model_key: str, res: Dict):
    plots = []
    for (layer, pos_name), (X, y) in res["data"].items():
        pca = PCA(n_components=2)
        X_pca = pca.fit_transform(X)

        tsne = TSNE(n_components=2, perplexity=min(10, max(2, len(X) // 3)), init="random", random_state=7)
        X_tsne = tsne.fit_transform(X)

        plots.append(((layer, pos_name), X_pca, X_tsne, y))

    if not plots:
        print(f"No plots for {model_key}")
        return

    cols = 2
    rows = math.ceil(len(plots) / cols)
    fig, axes = plt.subplots(rows, cols, figsize=(10, 4 * rows))
    if rows == 1:
        axes = np.array([axes])

    for ax, ((layer, pos_name), X_pca, _, y) in zip(axes.flatten(), plots):
        colors = np.where(y == "Alice", "tab:blue", "tab:orange")
        ax.scatter(X_pca[:, 0], X_pca[:, 1], c=colors, alpha=0.7, s=30)
        ax.set_title(f"{model_key} PCA: layer {layer}, pos {pos_name}")
        ax.set_xlabel("PC1")
        ax.set_ylabel("PC2")

    plt.tight_layout()
    plt.show()

    # t-SNE plots separately (can be slow)
    cols = 2
    rows = math.ceil(len(plots) / cols)
    fig, axes = plt.subplots(rows, cols, figsize=(10, 4 * rows))
    if rows == 1:
        axes = np.array([axes])

    for ax, ((layer, pos_name), _, X_tsne, y) in zip(axes.flatten(), plots):
        colors = np.where(y == "Alice", "tab:blue", "tab:orange")
        ax.scatter(X_tsne[:, 0], X_tsne[:, 1], c=colors, alpha=0.7, s=30)
        ax.set_title(f"{model_key} t-SNE: layer {layer}, pos {pos_name}")
        ax.set_xlabel("Dim1")
        ax.set_ylabel("Dim2")

    plt.tight_layout()
    plt.show()


for key, res in results.items():
    plot_pca_tsne(key, res)



def compute_separability(res: Dict) -> pd.DataFrame:
    rows = []
    for (layer, pos_name), (X, y) in res["data"].items():
        y_bin = (y == "Alice").astype(int)

        # Skip degenerate cases (only one class present)
        classes, counts = np.unique(y_bin, return_counts=True)
        if len(classes) < 2:
            continue
        if counts.min() < 2:
            continue

        n_splits = min(5, int(counts.min()))
        if n_splits < 2:
            continue

        cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=7)
        accs = []
        for train_idx, test_idx in cv.split(X, y_bin):
            # Extra safety: some folds can still collapse if extremely imbalanced
            if len(np.unique(y_bin[train_idx])) < 2 or len(np.unique(y_bin[test_idx])) < 2:
                continue
            clf = LogisticRegression(max_iter=1000)
            clf.fit(X[train_idx], y_bin[train_idx])
            preds = clf.predict(X[test_idx])
            accs.append(accuracy_score(y_bin[test_idx], preds))

        if not accs:
            continue

        rows.append({"layer": layer, "position": pos_name, "accuracy": float(np.mean(accs))})

    return pd.DataFrame(rows)


def plot_heatmap(model_key: str, df: pd.DataFrame):
    if df.empty:
        print(f"No separability data for {model_key}")
        return
    pivot = df.pivot(index="layer", columns="position", values="accuracy")
    fig, ax = plt.subplots(figsize=(6, 4))
    im = ax.imshow(pivot.values, cmap="viridis", vmin=0.5, vmax=1.0)
    ax.set_xticks(range(len(pivot.columns)))
    ax.set_xticklabels(pivot.columns)
    ax.set_yticks(range(len(pivot.index)))
    ax.set_yticklabels(pivot.index)
    ax.set_title(f"{model_key} separability (Alice vs Bob)")
    ax.set_xlabel("Token position")
    ax.set_ylabel("Layer")
    cbar = plt.colorbar(im, ax=ax)
    cbar.set_label("Accuracy")
    plt.tight_layout()
    plt.show()


separability = {key: compute_separability(res) for key, res in results.items()}
for key, df in separability.items():
    display(df)
    plot_heatmap(key, df)



def mean_difference_direction(X: np.ndarray, y: np.ndarray) -> np.ndarray:
    alice_mean = X[y == "Alice"].mean(axis=0)
    bob_mean = X[y == "Bob"].mean(axis=0)
    return alice_mean - bob_mean


def concept_direction_eval(res: Dict) -> pd.DataFrame:
    rows = []
    for (layer, pos_name), (X, y) in res["data"].items():
        # Skip degenerate cases
        if len(set(y.tolist())) < 2:
            continue
        direction = mean_difference_direction(X, y)
        proj = X @ direction
        threshold = np.median(proj)
        preds = (proj > threshold).astype(int)
        acc = accuracy_score((y == "Alice").astype(int), preds)
        rows.append({"layer": layer, "position": pos_name, "accuracy": float(acc)})
    return pd.DataFrame(rows)


def plot_concept_pca(model_key: str, res: Dict):
    """More interpretable than an arrow: show class centroids + projection hist."""
    df = compute_separability(res)
    if df.empty:
        return

    best = df.sort_values("accuracy", ascending=False).iloc[0]
    layer = int(best["layer"])
    pos_name = best["position"]
    X, y = res["data"][(layer, pos_name)]

    direction = mean_difference_direction(X, y)
    proj = X @ direction

    # PCA scatter with centroids
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X)

    colors = np.where(y == "Alice", "tab:blue", "tab:orange")
    plt.figure(figsize=(6, 4))
    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=colors, alpha=0.6, s=28)

    # Centroids in PCA space
    alice_cent = X_pca[y == "Alice"].mean(axis=0)
    bob_cent = X_pca[y == "Bob"].mean(axis=0)
    plt.scatter([alice_cent[0]], [alice_cent[1]], c="tab:blue", s=140, marker="X", edgecolor="k", linewidth=0.5, label="Alice centroid")
    plt.scatter([bob_cent[0]], [bob_cent[1]], c="tab:orange", s=140, marker="X", edgecolor="k", linewidth=0.5, label="Bob centroid")

    # Line connecting centroids (visual "axis" in PCA plane)
    plt.plot([alice_cent[0], bob_cent[0]], [alice_cent[1], bob_cent[1]], color="k", linewidth=1.5, alpha=0.8)

    plt.title(f"{model_key}: PCA + centroids (layer {layer}, {pos_name})")
    plt.xlabel("PC1")
    plt.ylabel("PC2")
    plt.legend(loc="best", frameon=True)
    plt.tight_layout()
    plt.show()

    # 1D projection histogram along concept direction
    plt.figure(figsize=(6, 3))
    plt.hist(proj[y == "Alice"], bins=20, alpha=0.6, label="Alice", color="tab:blue", density=True)
    plt.hist(proj[y == "Bob"], bins=20, alpha=0.6, label="Bob", color="tab:orange", density=True)
    plt.title(f"{model_key}: projection on (Alice−Bob) direction")
    plt.xlabel("Projection value")
    plt.ylabel("Density")
    plt.legend(loc="best")
    plt.tight_layout()
    plt.show()


concept_results = {key: concept_direction_eval(res) for key, res in results.items()}
for key, df in concept_results.items():
    display(df)
    plot_concept_pca(key, results[key])



from pathlib import Path
import json
import numpy as np

out_dir = Path("outputs/geometric_structure")
out_dir.mkdir(parents=True, exist_ok=True)

summary = {}

for model_key, res in results.items():
    # res["data"] maps (layer, position) -> (X, y)
    arrays = {}
    meta = {
        "model_key": model_key,
        "model_name": res.get("model_name"),
        "layers": [int(x) for x in res.get("layers", [])],
        "positions": {k: [int(i) for i in v] for k, v in res.get("positions", {}).items()},
    }

    for (layer, pos_name), (X, y) in res["data"].items():
        k = f"layer{int(layer)}__pos{pos_name}"
        arrays[f"{k}__X"] = X.astype(np.float32)
        arrays[f"{k}__y"] = y.astype(str)

    npz_path = out_dir / f"{model_key}_layerpos_data.npz"
    np.savez_compressed(npz_path, **arrays)

    meta_path = out_dir / f"{model_key}_meta.json"
    meta_path.write_text(json.dumps(meta, indent=2))

    summary[model_key] = {
        "npz": str(npz_path),
        "meta": str(meta_path),
        "num_layerpos": len(res["data"]),
    }

(out_dir / "SUMMARY.json").write_text(json.dumps(summary, indent=2))
print("Saved to:", out_dir.resolve())
print(json.dumps(summary, indent=2))
