{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7426bec3-7dcc-42e8-af9e-7c596e5a4ea9",
      "metadata": {
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: accelerate in /home/fernando.je/.local/lib/python3.12/site-packages (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from accelerate) (25.0)\n",
            "Requirement already satisfied: psutil in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from accelerate) (5.9.0)\n",
            "Requirement already satisfied: pyyaml in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=2.0.0 in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from accelerate) (2.8.0)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from accelerate) (0.34.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from accelerate) (0.6.2)\n",
            "Requirement already satisfied: filelock in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.10.0)\n",
            "Requirement already satisfied: requests in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.5)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.9)\n",
            "Requirement already satisfied: setuptools in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (69.5.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
            "Requirement already satisfied: networkx in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.4.0 in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2024.6.2)\n"
          ]
        }
      ],
      "source": [
        "# Optional dependency check (avoid installing into /home on clusters)\n",
        "import importlib.util\n",
        "print(\"accelerate available:\", importlib.util.find_spec(\"accelerate\") is not None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "22019acc-6dcb-439e-9937-2957bec4e3f4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HF token:  ········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Fix cluster home quota: redirect caches (edit path if needed)\n",
        "os.environ.setdefault(\"ROLE_REP_CACHE_DIR\", f\"/projects/JeFeSpace/KLM/cache/{os.environ.get('USER','user')}/role-rep\")\n",
        "os.environ.setdefault(\"HF_HOME\", os.path.join(os.environ[\"ROLE_REP_CACHE_DIR\"], \"hf\"))\n",
        "os.environ.setdefault(\"HUGGINGFACE_HUB_CACHE\", os.path.join(os.environ[\"HF_HOME\"], \"hub\"))\n",
        "os.environ.setdefault(\"TRANSFORMERS_CACHE\", os.path.join(os.environ[\"HF_HOME\"], \"transformers\"))\n",
        "os.environ.setdefault(\"TORCH_HOME\", os.path.join(os.environ[\"ROLE_REP_CACHE_DIR\"], \"torch\"))\n",
        "os.environ.setdefault(\"XDG_CACHE_HOME\", os.path.join(os.environ[\"ROLE_REP_CACHE_DIR\"], \"xdg\"))\n",
        "\n",
        "# Avoid widget/progress-bar issues + HF xet\n",
        "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
        "os.environ[\"DISABLE_TQDM\"] = \"1\"\n",
        "os.environ[\"HF_HUB_DISABLE_XET\"] = \"1\"\n",
        "\n",
        "import getpass\n",
        "if not (os.environ.get(\"HF_TOKEN\") or os.environ.get(\"HUGGINGFACE_HUB_TOKEN\")):\n",
        "    os.environ[\"HF_TOKEN\"] = getpass.getpass(\"HF token: \")\n",
        "    os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = os.environ[\"HF_TOKEN\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d9fc804",
      "metadata": {},
      "source": [
        "# Speaker Role Representation: Geometric Structure Analysis\n",
        "\n",
        "**Goal:** Compare how two models (Qwen vs Llama) internally represent speaker identity in a long two‑person transcript.\n",
        "\n",
        "We treat **speaker identity (Alice vs Bob)** as the concept and probe activations across layers and token positions.\n",
        "Deliverables: PCA/t‑SNE, separability heatmaps, and concept directions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "560a0fc1",
      "metadata": {},
      "source": [
        "**Question:** Do models maintain distinct internal representations for different speakers over a long conversation?\n",
        "\n",
        "**Setup:** We build a long two‑person transcript (Alice vs Bob). Each token is labeled by the most recent speaker tag.\n",
        "We extract hidden states across layers and token positions (speaker tag, subject tokens, verbs, final token).\n",
        "\n",
        "**Analyses:**\n",
        "- **Geometry:** PCA/t‑SNE plots show whether Alice and Bob tokens cluster.\n",
        "- **Separability:** Linear classifiers measure how well a simple boundary separates speakers.\n",
        "- **Concept direction:** Mean‑difference vectors test if a consistent \"speaker axis\" exists.\n",
        "\n",
        "**Comparison:** Run the same pipeline on **Qwen** and **Llama** to compare where and how the concept emerges.\n",
        "\n",
        "**Takeaway:** The layer and position with the cleanest separation indicate where speaker identity is most strongly encoded.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4c82514f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x146371e361b0>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# %%capture\n",
        "# Core\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List\n",
        "\n",
        "# --- Cache + auth (prevents /home quota issues on clusters) ---\n",
        "# You can override explicitly with ROLE_REP_CACHE_DIR.\n",
        "ROLE_REP_CACHE_DIR = os.environ.get(\"ROLE_REP_CACHE_DIR\")\n",
        "if ROLE_REP_CACHE_DIR is None:\n",
        "    user = os.environ.get(\"USER\", \"user\")\n",
        "    if os.path.isdir(\"/projects/JeFeSpace/KLM\"):\n",
        "        ROLE_REP_CACHE_DIR = f\"/projects/JeFeSpace/KLM/cache/{user}/role-rep\"\n",
        "    elif os.path.isdir(\"/scratch\"):\n",
        "        ROLE_REP_CACHE_DIR = f\"/scratch/{user}/role-rep-cache\"\n",
        "\n",
        "if ROLE_REP_CACHE_DIR:\n",
        "    os.makedirs(ROLE_REP_CACHE_DIR, exist_ok=True)\n",
        "    os.environ.setdefault(\"HF_HOME\", os.path.join(ROLE_REP_CACHE_DIR, \"hf\"))\n",
        "    os.environ.setdefault(\"HUGGINGFACE_HUB_CACHE\", os.path.join(os.environ[\"HF_HOME\"], \"hub\"))\n",
        "    os.environ.setdefault(\"TRANSFORMERS_CACHE\", os.path.join(os.environ[\"HF_HOME\"], \"transformers\"))\n",
        "    os.environ.setdefault(\"TORCH_HOME\", os.path.join(ROLE_REP_CACHE_DIR, \"torch\"))\n",
        "    os.environ.setdefault(\"XDG_CACHE_HOME\", os.path.join(ROLE_REP_CACHE_DIR, \"xdg\"))\n",
        "\n",
        "# Reduce notebook widget/progress issues\n",
        "os.environ.setdefault(\"HF_HUB_DISABLE_PROGRESS_BARS\", \"1\")\n",
        "# Avoid HF \"xet\" download path (often problematic on clusters)\n",
        "os.environ.setdefault(\"HF_HUB_DISABLE_XET\", \"1\")\n",
        "\n",
        "# If you installed packages into project space (e.g., accelerate), add that site-packages BEFORE importing transformers.\n",
        "ROLE_REP_SITE_PACKAGES = os.environ.get(\n",
        "    \"ROLE_REP_SITE_PACKAGES\",\n",
        "    \"/projects/JeFeSpace/KLM/pip_local/lib/python3.12/site-packages\",\n",
        ")\n",
        "if os.path.isdir(ROLE_REP_SITE_PACKAGES) and ROLE_REP_SITE_PACKAGES not in sys.path:\n",
        "    sys.path.insert(0, ROLE_REP_SITE_PACKAGES)\n",
        "\n",
        "\n",
        "# If you exported HF_TOKEN/HUGGINGFACE_HUB_TOKEN in your shell, this will be picked up automatically.\n",
        "HF_TOKEN = os.environ.get(\"HF_TOKEN\") or os.environ.get(\"HUGGINGFACE_HUB_TOKEN\")\n",
        "\n",
        "# ML / viz\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Hugging Face\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Repro\n",
        "np.random.seed(7)\n",
        "torch.manual_seed(7)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "836c564c",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1747bb41",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Alice: Hey Bob, before we jump in, I wanted to revisit the design proposal.\\nBob: Sure, I skimmed it last night; the latency targets look ambitious.\\nAlice: The client asked for sub-200ms p95; I think batching can get us there.\\nBob: Batching helps, but the cache invalidation could become tricky.\\nAlice: We can constrain invalidation to product-level keys instead of per-user.\\nBob: That might reduce precision, though; would that impact personalization?\\nAlice: Some, but we can re-rank on the client side for the top 10 results.\\nBob: Okay, so you are proposing a hybrid: coarse cache, fine rerank.\\nAlice: Exactly. And we should log enough to measure drift each week.\\nBob: Logging is fine, but data retention policy caps at 30 days.\\nAlice: Right, I can summarize weekly aggregates and delete raw events.\\nBob: Great. Also, the new API endpoint needs a version bump.\\nAlice: v3 seems reasonable; we can keep v2 for a deprecation window.\\nBob: Then we need a migration guide; I can draft it.\\nAlice: Thanks. Another point: the search index rebuild takes 6 hours.\\nBob: Maybe we can parallelize by shard and compress the postings lists.\\nAlice: If we compress too much, we might slow decoding at query time.\\nBob: True; we could trade storage for CPU if latency budget allows.\\nAlice: We'll benchmark both. Also, what about adding synonyms?\\nBob: Synonyms help recall, but they increase false positives.\\nAlice: We'll tune the threshold and evaluate per-category.\\nBob: Sounds good. On another note, QA reported flaky tests.\\nAlice: I saw that; I think the mock clock isn't resetting in CI.\\nBob: I can isolate those tests and add a fixture.\\nAlice: Appreciate it. Lastly, are we aligned on the rollout plan?\\nBob: Staged rollout: internal, then 5% external, then 50%.\\nAlice: And we monitor error rates and rollback if p95 spikes.\\nBob: Yes. I'll write the runbook.\\nAlice: Great, I'll update the proposal and send it today.\\nBob: Perfect; I will review as soon as it lands.\\nAlice: Thanks, Bob.\\nBob: Thanks, Alice.\""
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Transcript: long two-person dialogue (Alice/Bob)\n",
        "transcript_turns = [\n",
        "    \"Alice: Hey Bob, before we jump in, I wanted to revisit the design proposal.\",\n",
        "    \"Bob: Sure, I skimmed it last night; the latency targets look ambitious.\",\n",
        "    \"Alice: The client asked for sub-200ms p95; I think batching can get us there.\",\n",
        "    \"Bob: Batching helps, but the cache invalidation could become tricky.\",\n",
        "    \"Alice: We can constrain invalidation to product-level keys instead of per-user.\",\n",
        "    \"Bob: That might reduce precision, though; would that impact personalization?\",\n",
        "    \"Alice: Some, but we can re-rank on the client side for the top 10 results.\",\n",
        "    \"Bob: Okay, so you are proposing a hybrid: coarse cache, fine rerank.\",\n",
        "    \"Alice: Exactly. And we should log enough to measure drift each week.\",\n",
        "    \"Bob: Logging is fine, but data retention policy caps at 30 days.\",\n",
        "    \"Alice: Right, I can summarize weekly aggregates and delete raw events.\",\n",
        "    \"Bob: Great. Also, the new API endpoint needs a version bump.\",\n",
        "    \"Alice: v3 seems reasonable; we can keep v2 for a deprecation window.\",\n",
        "    \"Bob: Then we need a migration guide; I can draft it.\",\n",
        "    \"Alice: Thanks. Another point: the search index rebuild takes 6 hours.\",\n",
        "    \"Bob: Maybe we can parallelize by shard and compress the postings lists.\",\n",
        "    \"Alice: If we compress too much, we might slow decoding at query time.\",\n",
        "    \"Bob: True; we could trade storage for CPU if latency budget allows.\",\n",
        "    \"Alice: We'll benchmark both. Also, what about adding synonyms?\",\n",
        "    \"Bob: Synonyms help recall, but they increase false positives.\",\n",
        "    \"Alice: We'll tune the threshold and evaluate per-category.\",\n",
        "    \"Bob: Sounds good. On another note, QA reported flaky tests.\",\n",
        "    \"Alice: I saw that; I think the mock clock isn't resetting in CI.\",\n",
        "    \"Bob: I can isolate those tests and add a fixture.\",\n",
        "    \"Alice: Appreciate it. Lastly, are we aligned on the rollout plan?\",\n",
        "    \"Bob: Staged rollout: internal, then 5% external, then 50%.\",\n",
        "    \"Alice: And we monitor error rates and rollback if p95 spikes.\",\n",
        "    \"Bob: Yes. I'll write the runbook.\",\n",
        "    \"Alice: Great, I'll update the proposal and send it today.\",\n",
        "    \"Bob: Perfect; I will review as soon as it lands.\",\n",
        "    \"Alice: Thanks, Bob.\",\n",
        "    \"Bob: Thanks, Alice.\"\n",
        "]\n",
        "\n",
        "transcript = \"\\n\".join(transcript_turns)\n",
        "transcript"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c46bcdec",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model config: Qwen 7B + Qwen 2.5 + Llama 3.1 8B\n",
        "# These are HF names; swap to local paths if you have them.\n",
        "\n",
        "MODEL_NAMES = {\n",
        "    \"qwen_7b\": os.environ.get(\"QWEN_7B_MODEL\", \"Qwen/Qwen2-7B-Instruct\"),\n",
        "    \"qwen_2_5\": os.environ.get(\"QWEN_25_MODEL\", \"Qwen/Qwen2.5-7B-Instruct\"),\n",
        "    # NOTE: official HF repo name often includes \"Meta-\" prefix\n",
        "    \"llama_3_1_8b\": os.environ.get(\"LLAMA_31_8B_MODEL\", \"meta-llama/Meta-Llama-3.1-8B-Instruct\"),\n",
        "}\n",
        "\n",
        "# Some clusters ship an older transformers that can't parse Llama-3.1 rope_scaling.\n",
        "# This shim overrides rope_scaling into the older (type,factor) format so the model can load.\n",
        "MODEL_LOAD_KWARGS = {\n",
        "    \"llama_3_1_8b\": {\n",
        "        \"rope_scaling\": {\"type\": \"linear\", \"factor\": 8.0},\n",
        "    }\n",
        "}\n",
        "\n",
        "# Layers to probe: 0, 25%, 50%, 75%, 100%\n",
        "LAYER_FRACTIONS = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
        "\n",
        "# Token positions to probe\n",
        "POSITION_SPECS = {\n",
        "    \"final\": {\"type\": \"final\"},\n",
        "    \"speaker_tag\": {\"type\": \"string\", \"values\": [\"Alice\", \"Bob\"]},\n",
        "    \"verb\": {\"type\": \"string\", \"values\": [\"am\", \"will\", \"can\", \"should\", \"think\", \"like\", \"want\"]},\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0ca173c7",
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ModelBundle:\n",
        "    name: str\n",
        "    tokenizer: AutoTokenizer\n",
        "    model: AutoModelForCausalLM\n",
        "\n",
        "\n",
        "def _best_dtype():\n",
        "    # Use bf16 on A100/H100 when available; otherwise fall back.\n",
        "    if torch.cuda.is_available():\n",
        "        try:\n",
        "            major, _minor = torch.cuda.get_device_capability(0)\n",
        "            if major >= 8:  # Ampere+\n",
        "                return torch.bfloat16\n",
        "        except Exception:\n",
        "            pass\n",
        "        return torch.float16\n",
        "    return torch.float32\n",
        "\n",
        "\n",
        "def _load_config_with_rope_compat(model_id: str, cache_dir: str | None):\n",
        "    \"\"\"Load config.json as dict, patch rope_scaling for older transformers, then build config.\"\"\"\n",
        "    import json\n",
        "    from transformers import AutoConfig\n",
        "    from transformers.utils import cached_file\n",
        "\n",
        "    cfg_path = cached_file(model_id, \"config.json\", cache_dir=cache_dir, token=HF_TOKEN)\n",
        "    with open(cfg_path, \"r\") as f:\n",
        "        cfg = json.load(f)\n",
        "\n",
        "    rs = cfg.get(\"rope_scaling\")\n",
        "    # Llama-3.1 uses a richer rope_scaling schema (rope_type/low_freq_factor/etc.)\n",
        "    # Older transformers expects exactly {type, factor}.\n",
        "    if isinstance(rs, dict) and (\"type\" not in rs) and (\"rope_type\" in rs):\n",
        "        cfg[\"rope_scaling\"] = {\"type\": \"linear\", \"factor\": float(rs.get(\"factor\", 8.0))}\n",
        "\n",
        "    return AutoConfig.from_dict(cfg)\n",
        "\n",
        "\n",
        "def load_model(name: str, model_kwargs: Dict | None = None) -> ModelBundle:\n",
        "    import importlib.util\n",
        "\n",
        "    cache_dir = os.environ.get(\"TRANSFORMERS_CACHE\") or os.environ.get(\"HF_HOME\")\n",
        "    dtype = _best_dtype()\n",
        "    model_kwargs = model_kwargs or {}\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        name,\n",
        "        cache_dir=cache_dir,\n",
        "        token=HF_TOKEN,\n",
        "    )\n",
        "\n",
        "    have_accelerate = importlib.util.find_spec(\"accelerate\") is not None\n",
        "    device_map = \"auto\" if (torch.cuda.is_available() and have_accelerate) else None\n",
        "    low_cpu_mem_usage = True if have_accelerate else False\n",
        "\n",
        "    try:\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            name,\n",
        "            cache_dir=cache_dir,\n",
        "            device_map=device_map,\n",
        "            torch_dtype=dtype,\n",
        "            low_cpu_mem_usage=low_cpu_mem_usage,\n",
        "            token=HF_TOKEN,\n",
        "            **model_kwargs,\n",
        "        )\n",
        "    except ValueError as e:\n",
        "        # Handle Llama-3.1 rope_scaling schema mismatch on older transformers\n",
        "        if \"rope_scaling\" in str(e) and \"type\" in str(e) and \"factor\" in str(e):\n",
        "            cfg = _load_config_with_rope_compat(name, cache_dir)\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                name,\n",
        "                cache_dir=cache_dir,\n",
        "                config=cfg,\n",
        "                device_map=device_map,\n",
        "                torch_dtype=dtype,\n",
        "                low_cpu_mem_usage=low_cpu_mem_usage,\n",
        "                token=HF_TOKEN,\n",
        "            )\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "    # If accelerate isn't available but CUDA is, move entire model to GPU.\n",
        "    if torch.cuda.is_available() and device_map is None:\n",
        "        model = model.to(\"cuda\")\n",
        "\n",
        "    # Ensure hidden states are returned\n",
        "    model.config.output_hidden_states = True\n",
        "    model.eval()\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    return ModelBundle(name=name, tokenizer=tokenizer, model=model)\n",
        "\n",
        "\n",
        "def select_layers(num_layers: int, fractions: List[float]) -> List[int]:\n",
        "    indices = []\n",
        "    for f in fractions:\n",
        "        idx = int(round(f * num_layers))\n",
        "        idx = max(0, min(num_layers, idx))\n",
        "        indices.append(idx)\n",
        "    return sorted(list(dict.fromkeys(indices)))\n",
        "\n",
        "\n",
        "def find_positions(tokens: List[str], spec: Dict) -> List[int]:\n",
        "    if spec[\"type\"] == \"final\":\n",
        "        return [len(tokens) - 1]\n",
        "    if spec[\"type\"] == \"string\":\n",
        "        values = set(spec[\"values\"])\n",
        "        hits = [i for i, t in enumerate(tokens) if t.strip() in values]\n",
        "        if hits:\n",
        "            return hits\n",
        "        return [i for i, t in enumerate(tokens) if t.strip().lstrip(\"Ġ\").lower() in {v.lower() for v in values}]\n",
        "    return []\n",
        "\n",
        "\n",
        "def label_speakers(tokens: List[str], raw_text: str) -> np.ndarray:\n",
        "    labels = []\n",
        "    last_speaker = None\n",
        "    cursor = 0\n",
        "\n",
        "    for tok in tokens:\n",
        "        window = raw_text[cursor: cursor + 200]\n",
        "        if \"Alice:\" in window and (\"Bob:\" not in window or window.index(\"Alice:\") < window.index(\"Bob:\")):\n",
        "            last_speaker = \"Alice\"\n",
        "        if \"Bob:\" in window and (\"Alice:\" not in window or window.index(\"Bob:\") < window.index(\"Alice:\")):\n",
        "            last_speaker = \"Bob\"\n",
        "\n",
        "        labels.append(last_speaker if last_speaker else \"Unknown\")\n",
        "\n",
        "        tok_clean = tok.replace(\"Ġ\", \" \")\n",
        "        if tok_clean in raw_text[cursor:]:\n",
        "            cursor = raw_text.index(tok_clean, cursor) + len(tok_clean)\n",
        "        else:\n",
        "            cursor = min(cursor + max(1, len(tok_clean)), len(raw_text))\n",
        "\n",
        "    return np.array(labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "067afae4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_analysis_for_model(bundle: ModelBundle) -> Dict:\n",
        "    tokenizer = bundle.tokenizer\n",
        "    model = bundle.model\n",
        "\n",
        "    encoded = tokenizer(transcript, return_tensors=\"pt\")\n",
        "    input_ids = encoded[\"input_ids\"]\n",
        "\n",
        "    # Try moving inputs to the model device when possible\n",
        "    try:\n",
        "        encoded = encoded.to(next(model.parameters()).device)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encoded, output_hidden_states=True, return_dict=True)\n",
        "\n",
        "    hidden_states = outputs.hidden_states\n",
        "    if hidden_states is None:\n",
        "        raise RuntimeError(\"Model did not return hidden states; ensure output_hidden_states=True\")\n",
        "\n",
        "    num_layers = len(hidden_states) - 1\n",
        "    layer_indices = select_layers(num_layers, LAYER_FRACTIONS)\n",
        "\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "    positions = {name: find_positions(tokens, spec) for name, spec in POSITION_SPECS.items()}\n",
        "\n",
        "    speaker_labels = label_speakers(tokens, transcript)\n",
        "\n",
        "    data = {}\n",
        "    for layer in layer_indices:\n",
        "        hs = hidden_states[layer][0].detach().cpu().numpy()  # [seq_len, hidden_dim]\n",
        "        for pos_name, pos_indices in positions.items():\n",
        "            pos_indices = [p for p in pos_indices if p < hs.shape[0]]\n",
        "            if not pos_indices:\n",
        "                continue\n",
        "            X = hs[pos_indices]\n",
        "            y = speaker_labels[pos_indices]\n",
        "            mask = np.isin(y, [\"Alice\", \"Bob\"])\n",
        "            X = X[mask]\n",
        "            y = y[mask]\n",
        "            if len(y) < 4:\n",
        "                continue\n",
        "            data[(layer, pos_name)] = (X, y)\n",
        "\n",
        "    return {\n",
        "        \"model_name\": bundle.name,\n",
        "        \"tokens\": tokens,\n",
        "        \"layers\": layer_indices,\n",
        "        \"positions\": positions,\n",
        "        \"speaker_labels\": speaker_labels,\n",
        "        \"data\": data,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d9ccf2c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading qwen_7b: Qwen/Qwen2-7B-Instruct\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading qwen_2_5: Qwen/Qwen2.5-7B-Instruct\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "# Run analysis model-by-model (avoids loading all 7B/8B models at once)\n",
        "\n",
        "results = {}\n",
        "for key, name in MODEL_NAMES.items():\n",
        "    print(f\"Loading {key}: {name}\")\n",
        "    bundle = load_model(name, model_kwargs=MODEL_LOAD_KWARGS.get(key))\n",
        "    try:\n",
        "        results[key] = run_analysis_for_model(bundle)\n",
        "    finally:\n",
        "        # Free GPU/CPU memory before next model\n",
        "        try:\n",
        "            del bundle.model\n",
        "        except Exception:\n",
        "            pass\n",
        "        del bundle\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "list(results.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbcdefb5",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_pca_tsne(model_key: str, res: Dict):\n",
        "    plots = []\n",
        "    for (layer, pos_name), (X, y) in res[\"data\"].items():\n",
        "        pca = PCA(n_components=2)\n",
        "        X_pca = pca.fit_transform(X)\n",
        "\n",
        "        tsne = TSNE(n_components=2, perplexity=min(10, max(2, len(X) // 3)), init=\"random\", random_state=7)\n",
        "        X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "        plots.append(((layer, pos_name), X_pca, X_tsne, y))\n",
        "\n",
        "    if not plots:\n",
        "        print(f\"No plots for {model_key}\")\n",
        "        return\n",
        "\n",
        "    cols = 2\n",
        "    rows = math.ceil(len(plots) / cols)\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(10, 4 * rows))\n",
        "    if rows == 1:\n",
        "        axes = np.array([axes])\n",
        "\n",
        "    for ax, ((layer, pos_name), X_pca, _, y) in zip(axes.flatten(), plots):\n",
        "        colors = np.where(y == \"Alice\", \"tab:blue\", \"tab:orange\")\n",
        "        ax.scatter(X_pca[:, 0], X_pca[:, 1], c=colors, alpha=0.7, s=30)\n",
        "        ax.set_title(f\"{model_key} PCA: layer {layer}, pos {pos_name}\")\n",
        "        ax.set_xlabel(\"PC1\")\n",
        "        ax.set_ylabel(\"PC2\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # t-SNE plots separately (can be slow)\n",
        "    cols = 2\n",
        "    rows = math.ceil(len(plots) / cols)\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(10, 4 * rows))\n",
        "    if rows == 1:\n",
        "        axes = np.array([axes])\n",
        "\n",
        "    for ax, ((layer, pos_name), _, X_tsne, y) in zip(axes.flatten(), plots):\n",
        "        colors = np.where(y == \"Alice\", \"tab:blue\", \"tab:orange\")\n",
        "        ax.scatter(X_tsne[:, 0], X_tsne[:, 1], c=colors, alpha=0.7, s=30)\n",
        "        ax.set_title(f\"{model_key} t-SNE: layer {layer}, pos {pos_name}\")\n",
        "        ax.set_xlabel(\"Dim1\")\n",
        "        ax.set_ylabel(\"Dim2\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "for key, res in results.items():\n",
        "    plot_pca_tsne(key, res)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45ac8c02",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_separability(res: Dict) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    for (layer, pos_name), (X, y) in res[\"data\"].items():\n",
        "        y_bin = (y == \"Alice\").astype(int)\n",
        "        n_splits = min(5, len(y_bin))\n",
        "        if n_splits < 2:\n",
        "            continue\n",
        "        cv = StratifiedKFold(n_splits=n_splits)\n",
        "        accs = []\n",
        "        for train_idx, test_idx in cv.split(X, y_bin):\n",
        "            clf = LogisticRegression(max_iter=1000)\n",
        "            clf.fit(X[train_idx], y_bin[train_idx])\n",
        "            preds = clf.predict(X[test_idx])\n",
        "            accs.append(accuracy_score(y_bin[test_idx], preds))\n",
        "        rows.append({\"layer\": layer, \"position\": pos_name, \"accuracy\": float(np.mean(accs))})\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "def plot_heatmap(model_key: str, df: pd.DataFrame):\n",
        "    if df.empty:\n",
        "        print(f\"No separability data for {model_key}\")\n",
        "        return\n",
        "    pivot = df.pivot(index=\"layer\", columns=\"position\", values=\"accuracy\")\n",
        "    fig, ax = plt.subplots(figsize=(6, 4))\n",
        "    im = ax.imshow(pivot.values, cmap=\"viridis\", vmin=0.5, vmax=1.0)\n",
        "    ax.set_xticks(range(len(pivot.columns)))\n",
        "    ax.set_xticklabels(pivot.columns)\n",
        "    ax.set_yticks(range(len(pivot.index)))\n",
        "    ax.set_yticklabels(pivot.index)\n",
        "    ax.set_title(f\"{model_key} separability (Alice vs Bob)\")\n",
        "    ax.set_xlabel(\"Token position\")\n",
        "    ax.set_ylabel(\"Layer\")\n",
        "    cbar = plt.colorbar(im, ax=ax)\n",
        "    cbar.set_label(\"Accuracy\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "separability = {key: compute_separability(res) for key, res in results.items()}\n",
        "for key, df in separability.items():\n",
        "    display(df)\n",
        "    plot_heatmap(key, df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ad9340f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def mean_difference_direction(X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
        "    alice_mean = X[y == \"Alice\"].mean(axis=0)\n",
        "    bob_mean = X[y == \"Bob\"].mean(axis=0)\n",
        "    return alice_mean - bob_mean\n",
        "\n",
        "\n",
        "def concept_direction_eval(res: Dict) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    for (layer, pos_name), (X, y) in res[\"data\"].items():\n",
        "        direction = mean_difference_direction(X, y)\n",
        "        proj = X @ direction\n",
        "        threshold = np.median(proj)\n",
        "        preds = (proj > threshold).astype(int)\n",
        "        acc = accuracy_score((y == \"Alice\").astype(int), preds)\n",
        "        rows.append({\"layer\": layer, \"position\": pos_name, \"accuracy\": float(acc)})\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "def plot_concept_pca(model_key: str, res: Dict):\n",
        "    # Visualize direction in PCA space for one example (highest separability layer/pos)\n",
        "    df = compute_separability(res)\n",
        "    if df.empty:\n",
        "        return\n",
        "    best = df.sort_values(\"accuracy\", ascending=False).iloc[0]\n",
        "    layer = int(best[\"layer\"])\n",
        "    pos_name = best[\"position\"]\n",
        "    X, y = res[\"data\"][(layer, pos_name)]\n",
        "\n",
        "    direction = mean_difference_direction(X, y)\n",
        "    pca = PCA(n_components=2)\n",
        "    X_pca = pca.fit_transform(X)\n",
        "    dir_pca = pca.transform(direction.reshape(1, -1))[0]\n",
        "\n",
        "    colors = np.where(y == \"Alice\", \"tab:blue\", \"tab:orange\")\n",
        "    plt.figure(figsize=(5, 4))\n",
        "    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=colors, alpha=0.7, s=30)\n",
        "    plt.arrow(0, 0, dir_pca[0], dir_pca[1], color=\"black\", width=0.01)\n",
        "    plt.title(f\"{model_key} concept direction (layer {layer}, {pos_name})\")\n",
        "    plt.xlabel(\"PC1\")\n",
        "    plt.ylabel(\"PC2\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "concept_results = {key: concept_direction_eval(res) for key, res in results.items()}\n",
        "for key, df in concept_results.items():\n",
        "    display(df)\n",
        "    plot_concept_pca(key, results[key])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c377da8",
      "metadata": {},
      "source": [
        "## Write-up Template (fill after running)\n",
        "\n",
        "**Key observations**\n",
        "- Which layer shows the clearest Alice/Bob separation?\n",
        "- Which position (speaker tags vs verbs vs final token) carries the cleanest signal?\n",
        "- Do Qwen and Llama differ in where the signal peaks?\n",
        "\n",
        "**Interpretation**\n",
        "- Does the representation look linearly separable (high accuracy), or more diffuse?\n",
        "- Is speaker identity localized to specific positions or distributed?\n",
        "- Does speaker identity emerge late (higher layers) or early?\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
