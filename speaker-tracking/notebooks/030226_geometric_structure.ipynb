{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3d9fc804",
      "metadata": {},
      "source": [
        "# Speaker Role Representation: Geometric Structure Analysis\n",
        "\n",
        "**Goal:** Compare how two models (Qwen vs Llama) internally represent speaker identity in a long two‑person transcript.\n",
        "\n",
        "We treat **speaker identity (Alice vs Bob)** as the concept and probe activations across layers and token positions.\n",
        "Deliverables: PCA/t‑SNE, separability heatmaps, and concept directions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "560a0fc1",
      "metadata": {},
      "source": [
        "**Question:** Do models maintain distinct internal representations for different speakers over a long conversation?\n",
        "\n",
        "**Setup:** We build a long two‑person transcript (Alice vs Bob). Each token is labeled by the most recent speaker tag.\n",
        "We extract hidden states across layers and token positions (speaker tag, subject tokens, verbs, final token).\n",
        "\n",
        "**Analyses:**\n",
        "- **Geometry:** PCA/t‑SNE plots show whether Alice and Bob tokens cluster.\n",
        "- **Separability:** Linear classifiers measure how well a simple boundary separates speakers.\n",
        "- **Concept direction:** Mean‑difference vectors test if a consistent \"speaker axis\" exists.\n",
        "\n",
        "**Comparison:** Run the same pipeline on **Qwen** and **Llama** to compare where and how the concept emerges.\n",
        "\n",
        "**Takeaway:** The layer and position with the cleanest separation indicate where speaker identity is most strongly encoded.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4c82514f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x145fd7a10>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# %%capture\n",
        "# Core\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List\n",
        "\n",
        "# --- Cache + auth (prevents /home quota issues on clusters) ---\n",
        "# You can override explicitly with ROLE_REP_CACHE_DIR.\n",
        "ROLE_REP_CACHE_DIR = os.environ.get(\"ROLE_REP_CACHE_DIR\")\n",
        "if ROLE_REP_CACHE_DIR is None:\n",
        "    user = os.environ.get(\"USER\", \"user\")\n",
        "    if os.path.isdir(\"/projects/JeFeSpace/KLM\"):\n",
        "        ROLE_REP_CACHE_DIR = f\"/projects/JeFeSpace/KLM/cache/{user}/role-rep\"\n",
        "    elif os.path.isdir(\"/scratch\"):\n",
        "        ROLE_REP_CACHE_DIR = f\"/scratch/{user}/role-rep-cache\"\n",
        "\n",
        "if ROLE_REP_CACHE_DIR:\n",
        "    os.makedirs(ROLE_REP_CACHE_DIR, exist_ok=True)\n",
        "    os.environ.setdefault(\"HF_HOME\", os.path.join(ROLE_REP_CACHE_DIR, \"hf\"))\n",
        "    os.environ.setdefault(\"HUGGINGFACE_HUB_CACHE\", os.path.join(os.environ[\"HF_HOME\"], \"hub\"))\n",
        "    os.environ.setdefault(\"TRANSFORMERS_CACHE\", os.path.join(os.environ[\"HF_HOME\"], \"transformers\"))\n",
        "    os.environ.setdefault(\"TORCH_HOME\", os.path.join(ROLE_REP_CACHE_DIR, \"torch\"))\n",
        "    os.environ.setdefault(\"XDG_CACHE_HOME\", os.path.join(ROLE_REP_CACHE_DIR, \"xdg\"))\n",
        "\n",
        "# Reduce notebook widget/progress issues\n",
        "os.environ.setdefault(\"HF_HUB_DISABLE_PROGRESS_BARS\", \"1\")\n",
        "# Avoid HF \"xet\" download path (often problematic on clusters)\n",
        "os.environ.setdefault(\"HF_HUB_DISABLE_XET\", \"1\")\n",
        "\n",
        "# If you exported HF_TOKEN/HUGGINGFACE_HUB_TOKEN in your shell, this will be picked up automatically.\n",
        "HF_TOKEN = os.environ.get(\"HF_TOKEN\") or os.environ.get(\"HUGGINGFACE_HUB_TOKEN\")\n",
        "\n",
        "# ML / viz\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Hugging Face\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Repro\n",
        "np.random.seed(7)\n",
        "torch.manual_seed(7)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "836c564c",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1747bb41",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Alice: Hey Bob, before we jump in, I wanted to revisit the design proposal.\\nBob: Sure, I skimmed it last night; the latency targets look ambitious.\\nAlice: The client asked for sub-200ms p95; I think batching can get us there.\\nBob: Batching helps, but the cache invalidation could become tricky.\\nAlice: We can constrain invalidation to product-level keys instead of per-user.\\nBob: That might reduce precision, though; would that impact personalization?\\nAlice: Some, but we can re-rank on the client side for the top 10 results.\\nBob: Okay, so you are proposing a hybrid: coarse cache, fine rerank.\\nAlice: Exactly. And we should log enough to measure drift each week.\\nBob: Logging is fine, but data retention policy caps at 30 days.\\nAlice: Right, I can summarize weekly aggregates and delete raw events.\\nBob: Great. Also, the new API endpoint needs a version bump.\\nAlice: v3 seems reasonable; we can keep v2 for a deprecation window.\\nBob: Then we need a migration guide; I can draft it.\\nAlice: Thanks. Another point: the search index rebuild takes 6 hours.\\nBob: Maybe we can parallelize by shard and compress the postings lists.\\nAlice: If we compress too much, we might slow decoding at query time.\\nBob: True; we could trade storage for CPU if latency budget allows.\\nAlice: We'll benchmark both. Also, what about adding synonyms?\\nBob: Synonyms help recall, but they increase false positives.\\nAlice: We'll tune the threshold and evaluate per-category.\\nBob: Sounds good. On another note, QA reported flaky tests.\\nAlice: I saw that; I think the mock clock isn't resetting in CI.\\nBob: I can isolate those tests and add a fixture.\\nAlice: Appreciate it. Lastly, are we aligned on the rollout plan?\\nBob: Staged rollout: internal, then 5% external, then 50%.\\nAlice: And we monitor error rates and rollback if p95 spikes.\\nBob: Yes. I'll write the runbook.\\nAlice: Great, I'll update the proposal and send it today.\\nBob: Perfect; I will review as soon as it lands.\\nAlice: Thanks, Bob.\\nBob: Thanks, Alice.\""
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Transcript: long two-person dialogue (Alice/Bob)\n",
        "transcript_turns = [\n",
        "    \"Alice: Hey Bob, before we jump in, I wanted to revisit the design proposal.\",\n",
        "    \"Bob: Sure, I skimmed it last night; the latency targets look ambitious.\",\n",
        "    \"Alice: The client asked for sub-200ms p95; I think batching can get us there.\",\n",
        "    \"Bob: Batching helps, but the cache invalidation could become tricky.\",\n",
        "    \"Alice: We can constrain invalidation to product-level keys instead of per-user.\",\n",
        "    \"Bob: That might reduce precision, though; would that impact personalization?\",\n",
        "    \"Alice: Some, but we can re-rank on the client side for the top 10 results.\",\n",
        "    \"Bob: Okay, so you are proposing a hybrid: coarse cache, fine rerank.\",\n",
        "    \"Alice: Exactly. And we should log enough to measure drift each week.\",\n",
        "    \"Bob: Logging is fine, but data retention policy caps at 30 days.\",\n",
        "    \"Alice: Right, I can summarize weekly aggregates and delete raw events.\",\n",
        "    \"Bob: Great. Also, the new API endpoint needs a version bump.\",\n",
        "    \"Alice: v3 seems reasonable; we can keep v2 for a deprecation window.\",\n",
        "    \"Bob: Then we need a migration guide; I can draft it.\",\n",
        "    \"Alice: Thanks. Another point: the search index rebuild takes 6 hours.\",\n",
        "    \"Bob: Maybe we can parallelize by shard and compress the postings lists.\",\n",
        "    \"Alice: If we compress too much, we might slow decoding at query time.\",\n",
        "    \"Bob: True; we could trade storage for CPU if latency budget allows.\",\n",
        "    \"Alice: We'll benchmark both. Also, what about adding synonyms?\",\n",
        "    \"Bob: Synonyms help recall, but they increase false positives.\",\n",
        "    \"Alice: We'll tune the threshold and evaluate per-category.\",\n",
        "    \"Bob: Sounds good. On another note, QA reported flaky tests.\",\n",
        "    \"Alice: I saw that; I think the mock clock isn't resetting in CI.\",\n",
        "    \"Bob: I can isolate those tests and add a fixture.\",\n",
        "    \"Alice: Appreciate it. Lastly, are we aligned on the rollout plan?\",\n",
        "    \"Bob: Staged rollout: internal, then 5% external, then 50%.\",\n",
        "    \"Alice: And we monitor error rates and rollback if p95 spikes.\",\n",
        "    \"Bob: Yes. I'll write the runbook.\",\n",
        "    \"Alice: Great, I'll update the proposal and send it today.\",\n",
        "    \"Bob: Perfect; I will review as soon as it lands.\",\n",
        "    \"Alice: Thanks, Bob.\",\n",
        "    \"Bob: Thanks, Alice.\"\n",
        "]\n",
        "\n",
        "transcript = \"\\n\".join(transcript_turns)\n",
        "transcript"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c46bcdec",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model config: Qwen 7B + Qwen 2.5 + Llama 3.1 8B\n",
        "# These are HF names; swap to local paths if you have them.\n",
        "\n",
        "MODEL_NAMES = {\n",
        "    \"qwen_7b\": os.environ.get(\"QWEN_7B_MODEL\", \"Qwen/Qwen2-7B-Instruct\"),\n",
        "    \"qwen_2_5\": os.environ.get(\"QWEN_25_MODEL\", \"Qwen/Qwen2.5-7B-Instruct\"),\n",
        "    \"llama_3_1_8b\": os.environ.get(\"LLAMA_31_8B_MODEL\", \"meta-llama/Llama-3.1-8B-Instruct\"),\n",
        "}\n",
        "\n",
        "# Layers to probe: 0, 25%, 50%, 75%, 100%\n",
        "LAYER_FRACTIONS = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
        "\n",
        "# Token positions to probe\n",
        "POSITION_SPECS = {\n",
        "    \"final\": {\"type\": \"final\"},\n",
        "    \"speaker_tag\": {\"type\": \"string\", \"values\": [\"Alice\", \"Bob\"]},\n",
        "    \"verb\": {\"type\": \"string\", \"values\": [\"am\", \"will\", \"can\", \"should\", \"think\", \"like\", \"want\"]},\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0ca173c7",
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ModelBundle:\n",
        "    name: str\n",
        "    tokenizer: AutoTokenizer\n",
        "    model: AutoModelForCausalLM\n",
        "\n",
        "\n",
        "def _best_dtype():\n",
        "    # Use bf16 on A100/H100 when available; otherwise fall back.\n",
        "    if torch.cuda.is_available():\n",
        "        try:\n",
        "            major, _minor = torch.cuda.get_device_capability(0)\n",
        "            if major >= 8:  # Ampere+\n",
        "                return torch.bfloat16\n",
        "        except Exception:\n",
        "            pass\n",
        "        return torch.float16\n",
        "    return torch.float32\n",
        "\n",
        "\n",
        "def load_model(name: str) -> ModelBundle:\n",
        "    cache_dir = os.environ.get(\"TRANSFORMERS_CACHE\") or os.environ.get(\"HF_HOME\")\n",
        "    dtype = _best_dtype()\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        name,\n",
        "        cache_dir=cache_dir,\n",
        "        token=HF_TOKEN,\n",
        "    )\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        name,\n",
        "        cache_dir=cache_dir,\n",
        "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "        dtype=dtype,\n",
        "        low_cpu_mem_usage=True,\n",
        "        token=HF_TOKEN,\n",
        "    )\n",
        "\n",
        "    # Ensure hidden states are returned\n",
        "    model.config.output_hidden_states = True\n",
        "    model.eval()\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    return ModelBundle(name=name, tokenizer=tokenizer, model=model)\n",
        "\n",
        "\n",
        "def select_layers(num_layers: int, fractions: List[float]) -> List[int]:\n",
        "    indices = []\n",
        "    for f in fractions:\n",
        "        idx = int(round(f * num_layers))\n",
        "        idx = max(0, min(num_layers, idx))\n",
        "        indices.append(idx)\n",
        "    return sorted(list(dict.fromkeys(indices)))\n",
        "\n",
        "\n",
        "def find_positions(tokens: List[str], spec: Dict) -> List[int]:\n",
        "    if spec[\"type\"] == \"final\":\n",
        "        return [len(tokens) - 1]\n",
        "    if spec[\"type\"] == \"string\":\n",
        "        values = set(spec[\"values\"])\n",
        "        hits = [i for i, t in enumerate(tokens) if t.strip() in values]\n",
        "        if hits:\n",
        "            return hits\n",
        "        return [i for i, t in enumerate(tokens) if t.strip().lstrip(\"Ġ\").lower() in {v.lower() for v in values}]\n",
        "    return []\n",
        "\n",
        "\n",
        "def label_speakers(tokens: List[str], raw_text: str) -> np.ndarray:\n",
        "    labels = []\n",
        "    last_speaker = None\n",
        "    cursor = 0\n",
        "\n",
        "    for tok in tokens:\n",
        "        window = raw_text[cursor: cursor + 200]\n",
        "        if \"Alice:\" in window and (\"Bob:\" not in window or window.index(\"Alice:\") < window.index(\"Bob:\")):\n",
        "            last_speaker = \"Alice\"\n",
        "        if \"Bob:\" in window and (\"Alice:\" not in window or window.index(\"Bob:\") < window.index(\"Alice:\")):\n",
        "            last_speaker = \"Bob\"\n",
        "\n",
        "        labels.append(last_speaker if last_speaker else \"Unknown\")\n",
        "\n",
        "        tok_clean = tok.replace(\"Ġ\", \" \")\n",
        "        if tok_clean in raw_text[cursor:]:\n",
        "            cursor = raw_text.index(tok_clean, cursor) + len(tok_clean)\n",
        "        else:\n",
        "            cursor = min(cursor + max(1, len(tok_clean)), len(raw_text))\n",
        "\n",
        "    return np.array(labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "067afae4",
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "def run_analysis_for_model(bundle: ModelBundle) -> Dict:\n",
        "    tokenizer = bundle.tokenizer\n",
        "    model = bundle.model\n",
        "\n",
        "    encoded = tokenizer(transcript, return_tensors=\"pt\")\n",
        "    input_ids = encoded[\"input_ids\"]\n",
        "\n",
        "    # Try moving inputs to the model device when possible\n",
        "    try:\n",
        "        encoded = encoded.to(next(model.parameters()).device)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encoded, output_hidden_states=True, return_dict=True)\n",
        "\n",
        "    hidden_states = outputs.hidden_states\n",
        "    if hidden_states is None:\n",
        "        raise RuntimeError(\"Model did not return hidden states; ensure output_hidden_states=True\")\n",
        "\n",
        "    num_layers = len(hidden_states) - 1\n",
        "    layer_indices = select_layers(num_layers, LAYER_FRACTIONS)\n",
        "\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "    positions = {name: find_positions(tokens, spec) for name, spec in POSITION_SPECS.items()}\n",
        "\n",
        "    speaker_labels = label_speakers(tokens, transcript)\n",
        "\n",
        "    data = {}\n",
        "    for layer in layer_indices:\n",
        "        hs = hidden_states[layer][0].detach().cpu().numpy()  # [seq_len, hidden_dim]\n",
        "        for pos_name, pos_indices in positions.items():\n",
        "            pos_indices = [p for p in pos_indices if p < hs.shape[0]]\n",
        "            if not pos_indices:\n",
        "                continue\n",
        "            X = hs[pos_indices]\n",
        "            y = speaker_labels[pos_indices]\n",
        "            mask = np.isin(y, [\"Alice\", \"Bob\"])\n",
        "            X = X[mask]\n",
        "            y = y[mask]\n",
        "            if len(y) < 4:\n",
        "                continue\n",
        "            data[(layer, pos_name)] = (X, y)\n",
        "\n",
        "    return {\n",
        "        \"model_name\": bundle.name,\n",
        "        \"tokens\": tokens,\n",
        "        \"layers\": layer_indices,\n",
        "        \"positions\": positions,\n",
        "        \"speaker_labels\": speaker_labels,\n",
        "        \"data\": data,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d9ccf2c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b6cc5ae619864a68ad952667736243dc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af86bf4f51d54a5d8aa777eb5d56cd34",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6756616f1514441999b983e753757b25",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/339 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c5b6bccce804977a88d3e8c5226dbc0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "48360d17abb24a6cb2a2e670d7d5852e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cfd9debdfe5f47ecbc6678d2e735afca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5bc285b343404868bbc413b7c8c30575",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f49bb5c13c3a41db9959944ff094152b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c16e5f5fbb846b3a908d503c3094259",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "99f0795d60f74f6caa935d7a753215b6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "86e588f5a95e4a368794911358ba76c6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f16ea5bf2975409cba9acc2523b41bd0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ddccdf1549a24eaa901d29d44a1bc083",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/339 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Run analysis model-by-model (avoids loading all 7B/8B models at once)\n",
        "\n",
        "results = {}\n",
        "for key, name in MODEL_NAMES.items():\n",
        "    print(f\"Loading {key}: {name}\")\n",
        "    bundle = load_model(name)\n",
        "    try:\n",
        "        results[key] = run_analysis_for_model(bundle)\n",
        "    finally:\n",
        "        # Free GPU/CPU memory before next model\n",
        "        try:\n",
        "            del bundle.model\n",
        "        except Exception:\n",
        "            pass\n",
        "        del bundle\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "list(results.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbcdefb5",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_pca_tsne(model_key: str, res: Dict):\n",
        "    plots = []\n",
        "    for (layer, pos_name), (X, y) in res[\"data\"].items():\n",
        "        pca = PCA(n_components=2)\n",
        "        X_pca = pca.fit_transform(X)\n",
        "\n",
        "        tsne = TSNE(n_components=2, perplexity=min(10, max(2, len(X) // 3)), init=\"random\", random_state=7)\n",
        "        X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "        plots.append(((layer, pos_name), X_pca, X_tsne, y))\n",
        "\n",
        "    if not plots:\n",
        "        print(f\"No plots for {model_key}\")\n",
        "        return\n",
        "\n",
        "    cols = 2\n",
        "    rows = math.ceil(len(plots) / cols)\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(10, 4 * rows))\n",
        "    if rows == 1:\n",
        "        axes = np.array([axes])\n",
        "\n",
        "    for ax, ((layer, pos_name), X_pca, _, y) in zip(axes.flatten(), plots):\n",
        "        colors = np.where(y == \"Alice\", \"tab:blue\", \"tab:orange\")\n",
        "        ax.scatter(X_pca[:, 0], X_pca[:, 1], c=colors, alpha=0.7, s=30)\n",
        "        ax.set_title(f\"{model_key} PCA: layer {layer}, pos {pos_name}\")\n",
        "        ax.set_xlabel(\"PC1\")\n",
        "        ax.set_ylabel(\"PC2\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # t-SNE plots separately (can be slow)\n",
        "    cols = 2\n",
        "    rows = math.ceil(len(plots) / cols)\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(10, 4 * rows))\n",
        "    if rows == 1:\n",
        "        axes = np.array([axes])\n",
        "\n",
        "    for ax, ((layer, pos_name), _, X_tsne, y) in zip(axes.flatten(), plots):\n",
        "        colors = np.where(y == \"Alice\", \"tab:blue\", \"tab:orange\")\n",
        "        ax.scatter(X_tsne[:, 0], X_tsne[:, 1], c=colors, alpha=0.7, s=30)\n",
        "        ax.set_title(f\"{model_key} t-SNE: layer {layer}, pos {pos_name}\")\n",
        "        ax.set_xlabel(\"Dim1\")\n",
        "        ax.set_ylabel(\"Dim2\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "for key, res in results.items():\n",
        "    plot_pca_tsne(key, res)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45ac8c02",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_separability(res: Dict) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    for (layer, pos_name), (X, y) in res[\"data\"].items():\n",
        "        y_bin = (y == \"Alice\").astype(int)\n",
        "        n_splits = min(5, len(y_bin))\n",
        "        if n_splits < 2:\n",
        "            continue\n",
        "        cv = StratifiedKFold(n_splits=n_splits)\n",
        "        accs = []\n",
        "        for train_idx, test_idx in cv.split(X, y_bin):\n",
        "            clf = LogisticRegression(max_iter=1000)\n",
        "            clf.fit(X[train_idx], y_bin[train_idx])\n",
        "            preds = clf.predict(X[test_idx])\n",
        "            accs.append(accuracy_score(y_bin[test_idx], preds))\n",
        "        rows.append({\"layer\": layer, \"position\": pos_name, \"accuracy\": float(np.mean(accs))})\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "def plot_heatmap(model_key: str, df: pd.DataFrame):\n",
        "    if df.empty:\n",
        "        print(f\"No separability data for {model_key}\")\n",
        "        return\n",
        "    pivot = df.pivot(index=\"layer\", columns=\"position\", values=\"accuracy\")\n",
        "    fig, ax = plt.subplots(figsize=(6, 4))\n",
        "    im = ax.imshow(pivot.values, cmap=\"viridis\", vmin=0.5, vmax=1.0)\n",
        "    ax.set_xticks(range(len(pivot.columns)))\n",
        "    ax.set_xticklabels(pivot.columns)\n",
        "    ax.set_yticks(range(len(pivot.index)))\n",
        "    ax.set_yticklabels(pivot.index)\n",
        "    ax.set_title(f\"{model_key} separability (Alice vs Bob)\")\n",
        "    ax.set_xlabel(\"Token position\")\n",
        "    ax.set_ylabel(\"Layer\")\n",
        "    cbar = plt.colorbar(im, ax=ax)\n",
        "    cbar.set_label(\"Accuracy\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "separability = {key: compute_separability(res) for key, res in results.items()}\n",
        "for key, df in separability.\n",
        "items():\n",
        "    display(df)\n",
        "    plot_heatmap(key, df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ad9340f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def mean_difference_direction(X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
        "    alice_mean = X[y == \"Alice\"].mean(axis=0)\n",
        "    bob_mean = X[y == \"Bob\"].mean(axis=0)\n",
        "    return alice_mean - bob_mean\n",
        "\n",
        "\n",
        "def concept_direction_eval(res: Dict) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    for (layer, pos_name), (X, y) in res[\"data\"].items():\n",
        "        direction = mean_difference_direction(X, y)\n",
        "        proj = X @ direction\n",
        "        threshold = np.median(proj)\n",
        "        preds = (proj > threshold).astype(int)\n",
        "        acc = accuracy_score((y == \"Alice\").astype(int), preds)\n",
        "        rows.append({\"layer\": layer, \"position\": pos_name, \"accuracy\": float(acc)})\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "def plot_concept_pca(model_key: str, res: Dict):\n",
        "    # Visualize direction in PCA space for one example (highest separability layer/pos)\n",
        "    df = compute_separability(res)\n",
        "    if df.empty:\n",
        "        return\n",
        "    best = df.sort_values(\"accuracy\", ascending=False).iloc[0]\n",
        "    layer = int(best[\"layer\"])\n",
        "    pos_name = best[\"position\"]\n",
        "    X, y = res[\"data\"][(layer, pos_name)]\n",
        "\n",
        "    direction = mean_difference_direction(X, y)\n",
        "    pca = PCA(n_components=2)\n",
        "    X_pca = pca.fit_transform(X)\n",
        "    dir_pca = pca.transform(direction.reshape(1, -1))[0]\n",
        "\n",
        "    colors = np.where(y == \"Alice\", \"tab:blue\", \"tab:orange\")\n",
        "    plt.figure(figsize=(5, 4))\n",
        "    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=colors, alpha=0.7, s=30)\n",
        "    plt.arrow(0, 0, dir_pca[0], dir_pca[1], color=\"black\", width=0.01)\n",
        "    plt.title(f\"{model_key} concept direction (layer {layer}, {pos_name})\")\n",
        "    plt.xlabel(\"PC1\")\n",
        "    plt.ylabel(\"PC2\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "concept_results = {key: concept_direction_eval(res) for key, res in results.items()}\n",
        "for key, df in concept_results.items():\n",
        "    display(df)\n",
        "    plot_concept_pca(key, results[key])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c377da8",
      "metadata": {},
      "source": [
        "## Write-up Template (fill after running)\n",
        "\n",
        "**Key observations**\n",
        "- Which layer shows the clearest Alice/Bob separation?\n",
        "- Which position (speaker tags vs verbs vs final token) carries the cleanest signal?\n",
        "- Do Qwen and Llama differ in where the signal peaks?\n",
        "\n",
        "**Interpretation**\n",
        "- Does the representation look linearly separable (high accuracy), or more diffuse?\n",
        "- Is speaker identity localized to specific positions or distributed?\n",
        "- Does speaker identity emerge late (higher layers) or early?\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "role-rep",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
