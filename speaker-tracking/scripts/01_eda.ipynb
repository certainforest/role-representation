{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "273670e1",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e2d4b79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import pandas as pd \n",
    "import re\n",
    "from pathlib import Path\n",
    "from docx import Document\n",
    "from datasets import load_dataset\n",
    "from importlib import reload\n",
    "from utils.eval import send_openrouter_request, models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f2cb064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load transcripts\n",
    "dir = Path('../data/transcripts')\n",
    "doc_paths = sorted(dir.glob('*.docx'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7819ce",
   "metadata": {},
   "source": [
    "# Eval\n",
    "\"Common sense\" checks using openrouter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ff0bd558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create string representing a single conversation\n",
    "doc = Document(doc_paths[0])\n",
    "\n",
    "paragraphs = []\n",
    "for d in doc.paragraphs:\n",
    "    # print(d.text)\n",
    "    paragraphs.append(d.text)\n",
    "\n",
    "conversation = ' '.join(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60cf345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating qwen/qwen3-8B\n",
      "evaluating qwen/qwen3-14B\n",
      "evaluating qwen/qwen3-32B\n",
      "evaluating google/gemma-2-9b-it\n"
     ]
    }
   ],
   "source": [
    "# eval.\n",
    "output = []\n",
    "for m in models.values():\n",
    "    print(f'evaluating {m}')\n",
    "    messages = [{'role': 'user', 'content': f'name the speakers in this conversation, and give 1 sentence describing their role: {conversation}'}]\n",
    "    response, reasoning, refusal, provider = send_openrouter_request(messages, model = m)\n",
    "    output.append({'model': m, 'response': response, 'reasoning': reasoning, 'refusal': refusal, 'provider': provider})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a5eff98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = '''This transcript contains speaker labels. Some labels refer to the same real person.\n",
    "\n",
    "Tasks:\n",
    "1) How many unique real participants are there?\n",
    "2) Which speaker labels are duplicates of the same person?\n",
    "3) For each duplicate, say which label it should be merged into.\n",
    "4) Give one short sentence describing each real participant.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1411908f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here’s a detailed answer to your four tasks based on the provided transcript:\n",
      "\n",
      "---\n",
      "\n",
      "### 1) How many unique real participants are there?\n",
      "\n",
      "There are **three unique real participants** in the transcript:\n",
      "\n",
      "- **Jasmine** (the interviewer/reporter from NBC News)\n",
      "- **Speaker 4 / Lynn** (the CEO and co-founder of Fireworks, also referred to as \"Lynn\" by Jasmine at the end)\n",
      "- **Speaker 1** (a person present at the start of the transcript, possibly a colleague or assistant of Jasmine, but not identified by name)\n",
      "\n",
      "However, based on the context and the end of the transcript, \"Speaker 1\" is likely Jasmine herself or a third participant who is not central to the main interview. The main conversation is between Jasmine and Lynn (Speaker 4). \"Speaker 3\" appears briefly at the very beginning but does not participate in the main discussion.\n",
      "\n",
      "But to be precise from the transcript:\n",
      "- **Jasmine** (clearly identified)\n",
      "- **Speaker 4 / Lynn** (identified as the CEO of Fireworks, and called \"Lynn\" at the end)\n",
      "- **Speaker 1** (unclear identity, possibly Jasmine or another colleague, but not further identified)\n",
      "\n",
      "However, for the main interview, only **two unique real people** are actively engaged: Jasmine and Lynn. Speaker 1 and Speaker 3 are either Jasmine or background participants not central to the main conversation.\n",
      "\n",
      "But to be conservative and include all who speak:\n",
      "- **Jasmine**\n",
      "- **Speaker 1** (possibly Jasmine or a third person)\n",
      "- **Speaker 2** (briefly speaks, but not identified)\n",
      "- **Speaker 3** (briefly speaks, but not identified)\n",
      "- **Speaker 4 / Lynn**\n",
      "\n",
      "But only Jasmine and Lynn are clearly identified by name and role. The others are not identified and may be duplicates or background participants.\n",
      "\n",
      "**Best answer:**  \n",
      "There are at least **two unique real participants** clearly identified by name and role: Jasmine and Lynn (Speaker 4). There may be up to five speaking voices, but only two are clearly identified as real people.\n",
      "\n",
      "---\n",
      "\n",
      "### 2) Which speaker labels are duplicates of the same person?\n",
      "\n",
      "From the transcript:\n",
      "\n",
      "- **Speaker 1** appears at the very beginning and seems to be Jasmine or a colleague, but is not identified by name.\n",
      "- **Jasmine** is explicitly named and is the interviewer.\n",
      "- **Speaker 2** and **Speaker 3** are brief interjections at the start, but not identified.\n",
      "- **Speaker 4** is explicitly identified as Lynn, the CEO of Fireworks.\n",
      "\n",
      "There is no explicit statement that any of the \"Speaker X\" labels refer to the same person except for the possibility that **Speaker 1** is Jasmine herself, given the context and the fact that Jasmine is the main interviewer and the conversation starts with her.\n",
      "\n",
      "So:\n",
      "- **Speaker 1 is likely a duplicate of Jasmine.**\n",
      "- The other speaker labels (2, 3) are too brief and not identified, so we cannot confirm they are duplicates.\n",
      "\n",
      "**Conclusion:**\n",
      "- **Speaker 1** should be merged into **Jasmine** (the interviewer/reporter).\n",
      "\n",
      "---\n",
      "\n",
      "### 3) For each duplicate, say which label it should be merged into.\n",
      "\n",
      "- **Speaker 1 → Jasmine**\n",
      "\n",
      "(All other speaker labels are either unidentified or clearly distinct individuals.)\n",
      "\n",
      "---\n",
      "\n",
      "### 4) Give one short sentence describing each real participant.\n",
      "\n",
      "- **Jasmine:** An AI reporter at NBC News, conducting an interview about open source models and AI infrastructure.\n",
      "- **Lynn (Speaker 4):** CEO and co-founder of Fireworks, an AI inference platform company, with extensive experience in AI infrastructure and open source projects like PyTorch.\n",
      "\n",
      "---\n",
      "\n",
      "**Summary Table:**\n",
      "\n",
      "| Speaker Label | Real Identity | Description |\n",
      "|---------------|--------------|-------------|\n",
      "| Jasmine       | Jasmine      | AI reporter at NBC News, interviewer |\n",
      "| Speaker 1     | Jasmine      | Likely the same as Jasmine (should be merged) |\n",
      "| Speaker 4     | Lynn         | CEO and co-founder of Fireworks, AI infrastructure expert |\n",
      "| Speaker 2/3   | Unidentified | Brief, unidentified participants (not central to main conversation) |\n",
      "\n",
      "---\n",
      "\n",
      "Let me know if you want a more detailed breakdown or if you want to include the minor speakers as separate entities. But based on the transcript, only Jasmine and Lynn are clearly identified as real, named participants.\n"
     ]
    }
   ],
   "source": [
    "messages = [{'role': 'user', 'content': f'{task}: {conversation}'}]\n",
    "\n",
    "response, reasoning, refusal, provider = send_openrouter_request(messages, model = 'allenai/olmo-3.1-32b-instruct')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9fddb1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "340aab7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6923aaff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('## Analysis of the Conversation\\n'\n",
      " '\\n'\n",
      " 'This conversation provides a fascinating glimpse into the current state of open-source AI, particularly within the context of large language models (LLMs). \\n'\n",
      " '\\n'\n",
      " '**Key Takeaways:**\\n'\n",
      " '\\n'\n",
      " '* **Open-source LLMs are gaining traction:** Lynn, CEO of Fireworks, highlights the increasing adoption of open-source models by both startups and established enterprises, citing their impressive '\n",
      " 'performance and cost-effectiveness.\\n'\n",
      " '* **The gap between open and closed models is shrinking:** Lynn argues that the quality of open-source models is rapidly catching up to closed models, driven by advancements in research and the '\n",
      " 'collaborative nature of the open-source community.\\n'\n",
      " '* **Fine-tuning is crucial:** While base models are becoming increasingly powerful, Lynn emphasizes the importance of fine-tuning for specific applications. This allows companies to tailor models '\n",
      " 'to their unique needs and achieve optimal performance.\\n'\n",
      " '* **RL training is a game-changer:** Lynn sees reinforcement learning (RL) as a key driver of innovation, enabling more specialized and effective model training.\\n'\n",
      " '* **Standardization is needed:** Lynn calls for greater standardization in the open-source AI toolchain to simplify development and reduce wasted effort.\\n'\n",
      " '\\n'\n",
      " '**Interesting Points:**\\n'\n",
      " '\\n'\n",
      " \"* **Fireworks' impressive scale:** Processing 10 trillion tokens per day and 180 requests per second, Fireworks is clearly a major player in the open-source AI space.\\n\"\n",
      " '* **Eval protocol:** This open-source project aims to streamline the model evaluation and comparison process, making it easier for developers to choose the best models for their needs.\\n'\n",
      " '* **The need for an \"Android equivalent\" standard:** Lynn\\'s analogy highlights the current fragmentation in the open-source AI ecosystem and the need for greater interoperability between tools.\\n'\n",
      " '\\n'\n",
      " '**Overall, this conversation paints a positive picture of the future of open-source AI. With continued advancements in model capabilities, training techniques, and tooling, open-source LLMs are '\n",
      " 'poised to play an increasingly important role in shaping the future of AI.**')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(df['response'][3], width = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b71d1fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the transcript, there are five speakers involved in this conversation:\n",
      "\n",
      "*   **Jasmine:** An AI reporter for NBC News who is conducting the interview about open-source models.\n",
      "*   **Speaker 1:** A representative from the PR agency Six Eastern who is coordinating the interview for her client, Lynn.\n",
      "*   **Lynn (labeled as Speaker 4):** The CEO and co-founder of Fireworks who is being interviewed about her company and the AI industry.\n",
      "*   **Speaker 2:** A participant who speaks briefly at the beginning of the call while getting their notes organized.\n",
      "*   **Speaker 3:** A participant who makes a humorous comment during the initial small talk before the interview begins. **Analyzing the Request**\n",
      "\n",
      "I've started breaking down the user's request. My focus is on understanding the core needs: speaker identification and the count. I'm prioritizing the primary and secondary goals initially. The tertiary goal hasn't been defined yet, so I'll address that later.\n",
      "\n",
      "\n",
      "**Refining Speaker Profiles**\n",
      "\n",
      "I'm deep in the speaker analysis. Jasmine's role is crystal clear. I've successfully extracted and framed her self-introduction into the required one-sentence description. Now I will analyze the remaining speakers to extract their roles, and ensure the format is consistent.\n",
      "\n",
      "\n",
      "**Evaluating the Dialogue**\n",
      "\n",
      "I am now focused on evaluating the dialogue for Speaker 2, Speaker 3, and Speaker 4. The initial labeling strategy seems straightforward. I will look for clues in their dialogue to determine roles. My primary goal is to provide concise descriptions for each speaker.\n",
      "\n",
      "\n",
      "**Determining Speaker Roles**\n",
      "\n",
      "I've just finalized the descriptions for Speaker 2 and Speaker 3. Speaker 2 is a participant with minimal involvement. Speaker 3 also contributes to the pre-interview chatter. Now, I'm ready to move on to Speaker 4, and then analyze the presence of Lynn in the call.\n",
      "\n",
      "\n",
      "**Concluding the Analysis**\n",
      "\n",
      "I've determined Speaker 4 is Lynn, the CEO and co-founder of Fireworks, confirming Jasmine's introduction. Lynn is the primary interview subject. All speakers have now been processed. I'm satisfied with the clarity and accuracy of the output I'm working on.\n",
      "\n",
      "\n",
      "**Summarizing Speaker Roles**\n",
      "\n",
      "I'm now putting together the final product. I've confirmed five speakers and crafted a one-sentence description for each: Jasmine, Speaker 1, Speaker 2, Speaker 3, and Lynn. The output will be formatted as requested: speaker name, and descriptive role. The focus now is on polishing the exact wording to ensure clarity and conciseness.\n",
      "\n",
      "\n",
      "**Structuring the Response**\n",
      "\n",
      "I'm now drafting the structured response, ensuring the format requested is followed. I will begin with the speaker count, then provide the list of speaker names with their roles. I'm focusing on clarity.\n",
      "\n",
      "\n",
      " Google\n"
     ]
    }
   ],
   "source": [
    "print(response, reasoning, provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33eed30",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b66f67",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91803012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker 1  12:32:55  \n",
      "Yes, I know I'm sorry about all the back and forth, but I Lynn's schedule can be crazy, so\n",
      "\n",
      "Jasmine   12:33:00  \n",
      "no worries. I can't even imagine\n",
      "\n",
      "Speaker 1  12:33:04  \n",
      "CEO things. I feel like every, every meeting that she could possibly be in gets put on her calendar, and then we have to prioritize.\n",
      "\n",
      "Jasmine   12:33:11  \n",
      "Yeah, no, you're doing work in the same No,\n",
      "\n",
      "Speaker 1  12:33:16  \n",
      "you are. There's like, too many PR people in the world.\n",
      "\n",
      "Speaker 2  12:33:21  \n",
      "Okay, cool. Let me just get my other notes. It's 75 Google document tabs,\n",
      "\n",
      "Jasmine   12:33:31  \n",
      "yeah, oh man, whenever we have interns or like, students, will be like, I want to shadow you and see what you do is like, you will see me open a Google document and panic and like, that's all you're gonna see.\n",
      "\n",
      "Speaker 1  12:33:44  \n",
      "Yeah, you want to shadow and see my 900 open tabs.\n",
      "\n",
      "Speaker 3  12:33:51  \n",
      "Like, I'm not sure what you guys think people like adults do, but like, it ain't that\n",
      "\n",
      "Speaker 1  12:33:58  \n",
      "so true. We'll give Lynn a couple minutes, and then if she doesn't hop on, I will call her. It's just, are you on the East Coast? Yeah, I am.\n",
      "\n",
      "Jasmine   12:34:10  \n",
      "Where are you based? I'm based out of New York.\n",
      "\n",
      "Speaker 1  12:34:13  \n",
      "Oh, nice. I'm in Greater Boston area. Oh, no, yeah, yeah. I don't know if you're familiar. Did you like go to school in area or anything?\n",
      "\n",
      "Jasmine   12:34:22  \n",
      "Yeah, I'm doing some research at MIT, so kind of, yeah, I like, was grew up a little bit there. Yeah, are you near Somerville, Cambridge or\n",
      "\n",
      "Speaker 1  12:34:32  \n",
      "Yeah, yeah, I'm actually, I'm in Stoneham, so I'm like, a little bit north of Somerville, but yeah, Sam Erin, yeah, to school in Boston. I did. I went to BU Boston\n",
      "\n",
      "Jasmine   12:34:44  \n",
      "University. Yeah, awesome. Good terriers.\n",
      "\n",
      "Speaker 1  12:34:47  \n",
      "That's great. I'm so glad you know that\n",
      "\n",
      "Jasmine   12:34:51  \n",
      "that's awesome. How did you end up working at\n",
      "\n",
      "Speaker 1  12:34:54  \n",
      "fireworks? So I'm actually, I work for an agency. I work for six Eastern so we represent fireworks. We represent a couple other folks as well, in, like, open source AI and like, developer tools and things like that. But yeah, we started working with fireworks in, oh my gosh, like beginning of 2024, when they were like, tiny, tiny, tiny company. Now they're like, one of the biggest like inference, you know, platforms in the world, which is so\n",
      "\n",
      "Jasmine   12:35:21  \n",
      "cool, awesome, yeah, please send me a list of your other Yeah, yeah. Okay. Awesome. Hi. Lynn, hi. Lynn, hi. Great morning. Okay, yeah, sorry for interrupting while you're traveling. I hope it's going. Well, okay, let's see. Okay, so I'll just do a quick introduction of myself and then kind of explain the story we're looking at. So my name is Jasmine. I'm an AI reporter at NBC News, and right now I'm looking into open source models within the startup ecosystem, I saw that you're like doing like inference, and have a lot of different tools for people to work with. And so it was really curious to get your thoughts on, like how open source fits into that stack, and understand like the how different models compare and that kind of thing. So hopefully that gives a little more clarity, and would love if you could introduce yourself as well.\n",
      "\n",
      "Speaker 4  12:36:18  \n",
      "Yeah, so I'm lean CEO, co founder of fireworks, started fireworks about three years ago, and we have seven co founders, pretty big funding team, lot of experience building out AI infrastructure in the past 10 years in hyperscalers, like mostly from meta and also on Google.\n",
      "\n",
      "Jasmine   12:36:40  \n",
      "Oh, awesome, wow. What's, what's your background?\n",
      "\n",
      "Speaker 4  12:36:45  \n",
      "Yeah, so I've been, I've been working at meta for seven years and in the industry for 20 years. Yeah, yeah. So, couple of our co founders were like leading pytorch team at five years before we started this company. And obviously we are very, very passionate familiar with open source, because pytorch is a very well adopted. Open Source project has been the backlog for for many of the newer models, especially Jean models today,\n",
      "\n",
      "Jasmine   12:37:26  \n",
      "that's pretty love torch, no, that's so cool. I can't even believe in talking to someone who worked on that, that's awesome, I guess. Like, yeah, like, curious to understand more, like, how your team views open source in general. Like, what is the future of models like quen within the industry? Yeah.\n",
      "\n",
      "Speaker 4  12:37:43  \n",
      "So I think probably you also notice, starting from this year, the open models, not just LM, but also in other modalities, start to pick up. Especially pick up the pace, especially on the quality side. You can see these days, whenever people launch, have a model launch, whether it's open or closed, they compare with everybody and people, different companies, different model labs, close or open, they oscillate in on the leaderboard of who is better. So it's a very clear sign that between open and close, that the gap is really, really shrinking and and to some point our plateauing, or kind of converging to the same point in the future. So, so that's a clear training, and there's a fundamental reason why. And the closed labs are also like they are not shying away off like acknowledging, acknowledging the fact that it's all converging. First of all, the models is defined by multiple things. One is the training data. And training data are mostly coming from public internet or labeling companies. Almost all labs are using like calling the same internet, it's one same internet and also using the same labeling company. So there's not much difference there. And second is the researchers who train the model, and the researchers are moving around. If one person move around like They normalize, they bring, you know, the thinking and methodology. So then there's not really much secret sauce. I will say secret sauce mostly lies in data, and whoever has the deepest amount of high quality data, whichever company is going to have an upper hand. But regardless, I think the trend is is converging. That's where I believe. And if that's a trend, I would have much, much like open source community is going to bring up so much like open research, open science, open tools. It's going to build a vibrant ecosystem with a lot of participation from the industry. And I'm always kind of pro open open source Hospital.\n",
      "\n",
      "Jasmine   12:40:16  \n",
      "I'm curious. So that's one. Oh, sorry, you mentioned a labeling company like data annotation. Is there a specific one?\n",
      "\n",
      "Speaker 4  12:40:23  \n",
      "Oh, there are many of them. There are many, many of them. But my point is, they are being used by all different labs. Yeah. So I think the labeling company all specialize in certain domains. Yeah. So, but I am pretty sure, like, if one stand out in one domain, then all the labs are going to use them. So, yeah. So I think the future trend with this convergence, right? So, which is big, great news for all the developers. They have a lot more options and so on. But you can see that the models are more or less kind of get very close quality. The real differentiation, the real differentiation for application, is not on the model itself anymore. So here's the kind of backdrop of what's happening because of the advancement of all the jean models, it powers an enormous amount of creativity in user experience side, and that's where all the application we are. I We are watching extremely exciting experimentation of building and developing new applications, new agents, new user experiences. So if you think about many of the newer Ali native startups that just kind of rising up and becoming very quickly get to people are just counting 100 million, 100 million as have a new new bar of how fast company grows the but if you think About those companies, they're very young. They're like six month old, one year old, two years old. For young company, the product itself is not a mode, because it's very easy to copy product features, very easy. So you can have a UX researchers, a product manager, map out a competitor's product and just kind of pixel copy, but the real, real gist of those companies are driving new experimentation and leading impact is their understanding of their user. The deeper the understanding, the more the. Can feed that understanding back into continuous product development, and that velocity is the mode, and that velocity is powered by a lot of data locked inside the application. And this is not new to the AI. This has been always the case for after we pivot to mobile first from, from desktop, as in, we can access consumers, can access application anywhere, anywhere, anytime. And it kind of create a new slope of like product development. And that's where it's kind of my understanding user intent, understanding user engagement pattern, understand user preferences becomes a key as understanding that at scale, become the key of the successful product. So my my thesis is it's no longer the model that is going to differentiate application. It is application itself, an application in a model, co design, that is going to differentiate application. So what specific means is, is the direction we are passionate about the fireworks. Are passionate about it's called artificial autonomous intelligence. So this is fundamentally different from AGI, where we do not think about one size fits all. We think about model, not one size fits all, but the model, one size fits one any. Every application for every use case should have a model catered tuned towards towards their use case and application, and it's completely possible. So that's and that you should be fully automated. So that's why, that's what we mean we think about artificial autonomous intelligence. So to do that, it's not easy today, because we do not have a full tool chain to manage the full, adapt, tuning in, the customization, tailor, tailoring from model to the patient itself. And there's a lot to build. The whole that's where, again, I goes back to open open source ecosystem. There's so many open source project that's vibrate in covering parts of these adaptation and customization, tailoring, and with ours, we want to kind of be the just do the boring plumbing work to tie these all together So concretely, what that means is we will like to create a feedback loop from the or not just fireworks in general. I think the industry should have that. Industry should have a feedback loop from product data, just understanding the user preferences, intention, engagement pattern, so on, and from that data to curate a model that is fully aligned with that data, and therefore being further adapted by the application surface, much better user experience because of that and that drive more user engagement, more data, better model quality, better better product. So we really think the industry should move towards that direction. That's not new. It has been were adopted in before. Jean AI actually, in the kind of machine learning era, the machine learning powered part of many of those part already have those feedback loop. So yeah. So that's kind of the My take of thinking about the open models.\n",
      "\n",
      "Jasmine   12:46:56  \n",
      "Where do you think we are today with, like, open source models, like, especially with companies like, how many of them are using, using these? I know Airbnb, like, just talked about using coin in production, right?\n",
      "\n",
      "Speaker 4  12:47:10  \n",
      "So I think many companies are actually actively experimenting open models, as well as bring them to production. So and when they use they typically don't use off the shelf open model. They typically would, as I said, would tune the models, tailor the models towards their usage pattern, maximize the quality, minimizes the latency and drive the best cost efficiency for their production workload to scale quickly to millions of developers or billions of consumers. So. So that is actually happening. But more importantly, it gives, it gives those company much more control, much, much more control, as in they can the base model is no longer a concern or a what they really need is kind of full control of the weights and they, after they tune, they because they they are also they have lots of internal considerations. Either it's regulatory or it's some kind of safety guardrail in the middle of those is going to be tailored. Every company is different, and have the control of changing things based on their own internal needs, which can change over time as well. Can change based on their outlet, to their customer, their customer needs. Everything is adapting and being very dynamic habitat control is extremely important.\n",
      "\n",
      "Jasmine   12:48:58  \n",
      "Does base How much does base model quality? I guess, like matter or unlock adoption. Like, for instance, with Bloomberg GPT, they're like, we use SFT, and we use a lot of high quality Bloomberg data. But in the end, like, even on financial tasks, sometimes it struggles compared to like, like GPT, three, five or like, if I do like SFT, I noticed sometimes like the model will like over index. How do we overcome, I guess, like, these types of problems, or is this becoming less of an issue as base models improve?\n",
      "\n",
      "Speaker 4  12:49:31  \n",
      "Yeah, so base model, especially this year, the leap of improvement lies in logical reasoning capability. So you can think about the model as analogy to human being, right? So the models IQ significant improved, and then the rest of tuning is more like teaching a model to be a specialist. It's like we all come we were born, we come with like the IQ of our of individual doesn't change over time, but we get educated, right? So, and we have special study on all these, education is not going to change our IQ, but more have our focus on practice certain things and be a domain expert. So the tuning of the model is not going to change the model's IQ most of the time, especially the recently, the resource learning technology is going to help the model explore a wide range of the possibilities. And in the teacher model, by Give, give it positive reward or negative reward. It's exactly like how, like we teach a child, right? So in the in the move, move the model to be really an expert in solving certain problems, and that technology has been proven to be a very effective so so I what I will say is, across the board, not just open model, across all the models start To have much, much strong reasoning capability and much strong kind of overall quality, and that baseline shift is extremely important for its adoption in enterprises, especially not just enterprise, but also native startups or digital native enterprises. So and then on top of that, the tuning will be just specialization, specialized, specialized in solving certain kind of application specific task very well. And that specialization is going to bring a lot of benefit, as in, the quality can be better. It will be much more interactive with real time latency. It can be much more cost efficient, because you can specialize a much smaller open model and to be cost efficient. So, so that's a trend. I think it's happening throughout this year, and we definitely see a lot more, a lot more appetite to work with an open model, to tune it and all full control over it\n",
      "\n",
      "Jasmine   12:52:20  \n",
      "is there, like a number you can put on that increasing appetite.\n",
      "\n",
      "Speaker 4  12:52:26  \n",
      "I I cannot put a number on the whole entire industry. Just number for fireworks, we are processing more than 10 trillion tokens a day, so I it's a very big number, and we're also processing 180 requests per second. That volume is in the in a ballpark, same ballpark. Of Google search traffic volume. So I'm very curious about our tokens per day number, 10 channel tokens per day number, because I think open AI, they had a dev day two weeks ago. They announced their API traffic number, somehow it's smaller than our number. I don't believe that they probably made a mistake in their numbers, but I think the magnitude My point is the it is serious. People are actually adopting and experimenting and adopting open modeling in in production in a very serious way, all the way from Ai native startups to digital native enterprises, and also some traditional enterprise as well.\n",
      "\n",
      "Jasmine   12:53:43  \n",
      "Oh, wow. Are there any traditional ones that you're aware of that I should be that I should be aware of? Yeah.\n",
      "\n",
      "Speaker 4  12:53:50  \n",
      "So we, um, we work. I cannot share their logos, and so double check with my marketing team if we have marketing rights. But just give you some sense of the kind of industry we work with. We have onboarded several insurance companies. We have onboarded banks, financial institutions. We have onboarded healthcare companies, multiple of them. We have onboarded a food chain company and yeah, so it is actually, I will, I wouldn't expect that in usually, I thought kind of the traditional enterprise will wait a little bit to see how the overall industry is moving before they embrace and and we already have those customers awarded with that said, the majority of customers are AI native Startups are and digital native enterprises. And, yeah, so, but it's very exciting to see the whole entire industry is very energetic and very active in thinking about transforming their business using\n",
      "\n",
      "Jasmine   12:55:07  \n",
      "Jean. Ai. Are there certain types of models that are particularly popular? One thing we're seeing is there's a lot of Chinese models like quen that are really just, I mean, I personally love quin. So, but are there any particular reason you love quin? The Capybara is very cute, and then quen multi mold, the training, the training regimen was very cool. It was very smart. I started learning more recently and doing some research. So it's really cool. But yeah, is there? Are there any that are particularly popular, like with your customers?\n",
      "\n",
      "Speaker 4  12:55:40  \n",
      "Um, so I think different model has different strength. I think all the our customers are exploring all the models. I think the interesting thing is, where we want to help them is they keep asking us our opinion, which is worth trying certain kind of models. And first of all, we have all the models we like. Whenever a good state of art model launch, we launch very quickly, so our customer can, they can experiment with that, but it is a little bit a lot of work to keep trying and testing new models. And we build a two chain. It's called eval protocol, to it's open it's open source project. Also that, Oh, wow. Going back to our root of open source, eval dash protocol. That's cool. Yeah. So the idea there is, we, we want people to easily build their so, you know, Ling arena, right? So it's a public arena for, you know, people can try and kind of battle, and there's even low score and so on. But within each, every single company, they have their own private test suite. So we want to use Eva portal to enable their private Ali arena. So like very, very low, low effort, whenever there's a model launched, they will get a good understanding of whether it's worth pursuing or not. At the same time, eval protocol is also a linchpin between so Now many people are going to a new way of tuning is RL based tuning. The complexity is there a lot of trainers you can explore in open open source like VRL, RLM, slot, open pipe. There's so many open source ones. There's also various different kind of environments you. You need to kind of integrate with and run your RL process with. So Eva protocol is the LinkedIn connecting the wide set of environments with wide set of trainers. And we also firewalls also provide our trainers. So we will kind of provide our trainer as one of one of them. So, so yeah, so that's where we see, like lot of appetite in doing much more interesting tuning, especially in RL and and we want to provide a tool chain to help people leverage the best out of open source and in the continue their development in an easy way.\n",
      "\n",
      "Jasmine   12:58:36  \n",
      "Interesting is that kind of infrastructure. What we need to lower the barrier for, for using open source models, it seems like they're getting smaller with, like, mixture of experts models. And now there's the the eval, like, tool suite that you've you've mentioned. So like, what else do we need, or is missing for companies to, I guess, like, adopt this even more. Like, make it easier for them to adopt.\n",
      "\n",
      "Speaker 4  12:58:59  \n",
      "Yeah, I do think there will be a proliferation of tools across industry, and the tools don't really if you look at an end to end development life cycle, it's never one tool. It's a collection of many, many tools. I think the compatibility across tools are very, very important. So and the tools need to speak to each other too. So the developers, when they pick and choose various different tools based on their needs, it's very easy to assemble the end to end. So oftentimes people are like, spend a lot of time doing the plumbing work, which is, I think, just waste of time. And that's where we want to, like, develop this open source project Eva protocol, to connect the dots with everyone. With that said, I have full confidence in the open source community that they will many of the open source developer they will cover various different parts of the end to end, and in the totality, will be very, very comprehensive. But those tools need to work with each other. That's very important. So we really want to kind of push for standard or protocol in the open tool space for for to connect, to make the tuning part very easy, to make the customization part. So every single company should have their model that process super easy. And right now, there's no no one standard. It's kind of all over the place. And that's where we want to we're working with many open open source tooling provider to kind of standardize the integration part.\n",
      "\n",
      "Jasmine   13:00:44  \n",
      "Interesting, okay, this is my last question, I promise is a little bit like, just out of curiosity, what do you think is like the potential upset of like RL training? Do you think that it like is introducing new capabilities into models, or more, just like eliciting what is already inside of them.\n",
      "\n",
      "Speaker 4  13:01:04  \n",
      "So RL specifically is more like it is a selection process, but a selection process is selecting from the range of possibility a model can do. It doesn't select if the model cannot do certain kind of things fundamentally, then RL cannot push it out. So. So, for example, the model has a very strong logical reasoning. Overall, it can do two calls, but it doesn't do specific type of two calls very, very well. And then you can tune it very well towards that, if it, it can, it's, it basically, again, goes back to if the models, raw capability is very high, then it has a lot of chance that RL can focus the model to solve a particular problem really, really well and be the general purpose model. And that's where RLS technology is really suitable for, for what I call this kind of customer application tailored tuning. So that's why firewalls, as a provider, we are heavily investing in kind of making this super easy for developers. But with that side, I think we are going to solve a part of the puzzle of overall end to end developer, like from tuning to inference, we are significantly investing kind of integrating with the rest of the open source ecosystem to make the experience developers. Been super easy. Again, I think we like to solve the large scale system infrastructure and user experience problem, but there's a bigger problem of standardized integration, like we miss a kind of Android equivalent standard for for the jean AI development across industry, and I definitely think we need that. And like our eval protocol is a small step towards this type of standardization, but I will call for more participation across industry to standardize more. It will be a pure benefit to all developers if we can focus on, focus their energy on kind of streamline the whole entire trend. Otherwise, there's just too much wasted energy across the industry. People doing similar things, developers doing similar plumbing all the time. It's totally that kind of overhead should be removed.\n",
      "\n",
      "Jasmine   13:04:06  \n",
      "Thank you so much for your time. This was really great. If I have more questions, could I reach back out? Definitely. Yeah, love to do that. Yeah. Thank you so much, Erin. I've learned so much, and really appreciate.\n",
      "\n",
      "Transcribed by https://otter.ai\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e57edd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
